{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **پروژه پایانی - پیاده سازی الگوریتم های یادگیری ماشین بر روی داده های کووید**","metadata":{}},{"cell_type":"markdown","source":"## گام اول: آماده سازی داده ها ","metadata":{}},{"cell_type":"markdown","source":" ابتدا کتابخانه های مورد نیاز و  فایل دیتاست را لود میکنیم ","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndata = pd.read_json(\"/kaggle/input/covid-patient-datasets/covid.json\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-16T17:15:20.015454Z","iopub.execute_input":"2022-01-16T17:15:20.016320Z","iopub.status.idle":"2022-01-16T17:15:20.041842Z","shell.execute_reply.started":"2022-01-16T17:15:20.016265Z","shell.execute_reply":"2022-01-16T17:15:20.041164Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"data.info()\ndata.head(500)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T17:15:20.043550Z","iopub.execute_input":"2022-01-16T17:15:20.044127Z","iopub.status.idle":"2022-01-16T17:15:20.079648Z","shell.execute_reply.started":"2022-01-16T17:15:20.044097Z","shell.execute_reply":"2022-01-16T17:15:20.078855Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"488 <br>\nسطر داده داریم که باید نرمالیزه شوند. چون سن باینری نمی شود و ستون # برای شمارنده هست باید حذف شود. و سایر داده ها به <br> \nno => 0  <br>\nyes => 1 <br>\nنبدیل شود\n","metadata":{}},{"cell_type":"code","source":"normalData = data.drop(['#',\"age\"],axis = 1)\nfor param in normalData:\n    normalData[param] = normalData[param].apply(lambda x: 0 if x=='no' else 1)\n    #normalData[param] = normalData[param].map(dict(yes=1, no=0))\nnormalData.head(500)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T17:15:20.080806Z","iopub.execute_input":"2022-01-16T17:15:20.081116Z","iopub.status.idle":"2022-01-16T17:15:20.128685Z","shell.execute_reply.started":"2022-01-16T17:15:20.081089Z","shell.execute_reply":"2022-01-16T17:15:20.127860Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"حالا باید داده های تکراری را حذف کنیم","metadata":{}},{"cell_type":"code","source":"normalData.drop_duplicates(inplace=True)\nnormalData.head(500)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T17:15:20.129800Z","iopub.execute_input":"2022-01-16T17:15:20.130663Z","iopub.status.idle":"2022-01-16T17:15:20.156631Z","shell.execute_reply.started":"2022-01-16T17:15:20.130630Z","shell.execute_reply":"2022-01-16T17:15:20.155995Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"از 487 داده\n<br>\n487 - 285 = 202 \n<br>\nتکراری بودند\n<br>\nحال تمامی این داده ها دارای برچسب 1 هستند که باید این ویژگی به داده ها اضافه شود\n","metadata":{}},{"cell_type":"code","source":"normalData[\"result\"] = 1\nnormalData.head(500)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T17:15:20.158157Z","iopub.execute_input":"2022-01-16T17:15:20.158733Z","iopub.status.idle":"2022-01-16T17:15:20.188904Z","shell.execute_reply.started":"2022-01-16T17:15:20.158707Z","shell.execute_reply":"2022-01-16T17:15:20.188123Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"حال باید تمامی ترکیبات را بدست بیاوریم و آن هاایی که در داده ها نیستند برچسب 0 بزنیم و با داده های قبلی ترکیب کنیم","metadata":{}},{"cell_type":"code","source":"from itertools import product\n\nuniques = [normalData[i].unique().tolist() for i in normalData.columns ]\n\n# conver lable to 0\nuniques[-1][0] = 0\nfalseData = pd.DataFrame(product(*uniques), columns = normalData.columns)\n\n\nfinalData = normalData.append(falseData)\nfinalData.head(5000000000)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T17:15:20.189952Z","iopub.execute_input":"2022-01-16T17:15:20.190211Z","iopub.status.idle":"2022-01-16T17:15:31.276402Z","shell.execute_reply.started":"2022-01-16T17:15:20.190186Z","shell.execute_reply":"2022-01-16T17:15:31.275582Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"الان داده هایی که تکراری که برچسب 0 دارند باید حذف بشوند","metadata":{}},{"cell_type":"code","source":"feature_cols = finalData.columns;\n#remove the result column\nfeature_cols =feature_cols[:-1]\n\nfinalData.drop_duplicates(feature_cols,inplace=True)\nfinalData.head(5000000000)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T17:15:31.277309Z","iopub.execute_input":"2022-01-16T17:15:31.278272Z","iopub.status.idle":"2022-01-16T17:15:32.211050Z","shell.execute_reply.started":"2022-01-16T17:15:31.278216Z","shell.execute_reply":"2022-01-16T17:15:32.209890Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"گام اول با جمع آوری داده و نرمال کردن و ایجاد رکیب های موچود تمام شد","metadata":{}},{"cell_type":"code","source":"finalData.info()\nfinalData.describe().transpose()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T17:15:32.212060Z","iopub.execute_input":"2022-01-16T17:15:32.212238Z","iopub.status.idle":"2022-01-16T17:15:32.968054Z","shell.execute_reply.started":"2022-01-16T17:15:32.212216Z","shell.execute_reply":"2022-01-16T17:15:32.967399Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"# گام دوم: اجرای الگوریتم های یادگیری ماشین","metadata":{}},{"cell_type":"markdown","source":"## 1- بدست آوردن 5 ویژگی کم اهمیت\nبرای این کار از آنتروپی برای محاسیه \n<br> gain <br> \nاستفاده میکنیم","metadata":{}},{"cell_type":"code","source":"from math import log2\n\n# calculate the entropy \ndef entropy(trueResultCount,falseResultCount):  \n    p1 = trueResultCount / (trueResultCount + falseResultCount)\n    p2 = falseResultCount / (trueResultCount + falseResultCount)\n    return -((p1) * log2(p1) + (p2) * log2(p2))\n\n# calculate the gain of item \ndef gain(atr):\n    trueResultCount = finalData[finalData[atr] == 1][atr].count() \n    falseResultCount = finalData[finalData[atr] == 0][atr].count() \n    \n    trueWhenResultIsTrueCount = finalData.loc[(finalData[atr] == 1) & (finalData['result'] == 1)][atr].count()\n    falseWhenResultIsTrueCount = finalData.loc[(finalData[atr] == 0) & (finalData['result'] == 1)][atr].count() \n    trueWhenResultIsFalseCount = finalData.loc[(finalData[atr] == 1) & (finalData['result'] == 0)][atr].count()\n    falseWhenResultIsFalseCount = finalData.loc[(finalData[atr] == 0) & (finalData['result'] == 0)][atr].count() \n    \n    return entropyS - ((trueWhenResultIsTrueCount + trueWhenResultIsFalseCount) / (trueResultCount + falseResultCount)) * entropy(trueWhenResultIsTrueCount, trueWhenResultIsFalseCount) + ((falseWhenResultIsTrueCount + falseWhenResultIsFalseCount) / (trueResultCount + falseResultCount)) * entropy(falseWhenResultIsTrueCount, falseWhenResultIsFalseCount)\n  \n    \n #### ------------------ ####   \n    \ntrueResultCount = finalData[finalData.result == 1].result.count()  \nfalseResultCount = finalData[finalData.result == 0].result.count()\n\n## system entropy\nentropyS = round(entropy(trueResultCount, falseResultCount),8)\n\n\n## list of gain attributes\ngainList = []\nfor atr in feature_cols:\n    gainResult = gain(atr)\n    gainList.append([atr,round(gainResult, 8)])","metadata":{"execution":{"iopub.status.busy":"2022-01-16T17:15:32.969079Z","iopub.execute_input":"2022-01-16T17:15:32.969255Z","iopub.status.idle":"2022-01-16T17:15:41.916738Z","shell.execute_reply.started":"2022-01-16T17:15:32.969230Z","shell.execute_reply":"2022-01-16T17:15:41.915527Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"همان طور که در جدول آمده است 5 ویژگی آخر از کم اهمیت ترین ویژگی ها می باشند","metadata":{}},{"cell_type":"code","source":"print(\"system entropy: \",entropyS)\ndf = pd.DataFrame(gainList, columns =['attribute', 'gain']) \ndf = df.sort_values(by='gain', ascending=False)\ndf.head(50)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T17:15:41.918501Z","iopub.execute_input":"2022-01-16T17:15:41.918813Z","iopub.status.idle":"2022-01-16T17:15:41.934323Z","shell.execute_reply.started":"2022-01-16T17:15:41.918772Z","shell.execute_reply":"2022-01-16T17:15:41.933880Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"## 2- محاسبه الگوریتم find-S","metadata":{}},{"cell_type":"code","source":"#training function to implement find-s algorithm\ndef FindS(c,t):\n    for i, val in enumerate(t):\n        if val == 1:\n            specific_hypothesis = c[i].copy()\n            break\n             \n    for i, val in enumerate(c):\n        if t[i] == 1:\n            for x in range(len(specific_hypothesis)):\n                if val[x] != specific_hypothesis[x]:\n                    specific_hypothesis[x] = -1\n                else:\n                    pass\n    \n    # replace =1 with ? and return\n    return  [x if x != -1 else '?' for x in specific_hypothesis] ","metadata":{"execution":{"iopub.status.busy":"2022-01-16T17:15:41.935214Z","iopub.execute_input":"2022-01-16T17:15:41.935824Z","iopub.status.idle":"2022-01-16T17:15:41.953068Z","shell.execute_reply.started":"2022-01-16T17:15:41.935792Z","shell.execute_reply":"2022-01-16T17:15:41.952543Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# make concept and target (array)\ntarget = np.array(finalData.result) \nconcept = np.array(finalData.drop(['result'], axis=1))\n\n\n# making an array of all the attributes\nd = [None] * len(finalData.columns)\n\nh = FindS(concept,target)\nprint(h)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T17:15:41.954379Z","iopub.execute_input":"2022-01-16T17:15:41.954931Z","iopub.status.idle":"2022-01-16T17:15:43.167011Z","shell.execute_reply.started":"2022-01-16T17:15:41.954906Z","shell.execute_reply":"2022-01-16T17:15:43.166200Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"## 3- محاسبه الگوریتم CE\n","metadata":{}},{"cell_type":"code","source":"def CE(concepts, target): \n    \n    specific_h = concepts[0].copy()\n\n    print(\"Initialization of specific_h and general_h\")\n\n    print(\"specific_h: \",specific_h)\n\n    general_h = [[\"?\" for i in range(len(specific_h))] for i in range(len(specific_h))]\n\n    print(\"general_h: \",general_h)\n\n    print(\"concepts: \",concepts)\n\n    for i, h in enumerate(concepts):\n\n        if target[i] == \"yes\":\n\n            for x in range(len(specific_h)):\n\n                #print(\"h[x]\",h[x])\n\n                if h[x] != specific_h[x]:\n\n                    specific_h[x] = '?'\n\n                    general_h[x][x] = '?'\n\n        if target[i] == \"no\":\n\n            for x in range(len(specific_h)):\n\n                if h[x] != specific_h[x]:\n\n                    general_h[x][x] = specific_h[x]\n\n                else:\n\n                    general_h[x][x] = '?'\n\n    print(\"\\nSteps of Candidate Elimination Algorithm: \",i+1)\n\n    print(\"Specific_h: \",i+1)\n\n    print(specific_h,\"\\n\")\n\n    print(\"general_h :\", i+1)\n\n    print(general_h)\n\n    indices = [i for i, val in enumerate(general_h) if val == ['?', '?', '?', '?', '?', '?']]\n\n    print(\"\\nIndices\",indices)\n\n    for i in indices:\n\n        general_h.remove(['?', '?', '?', '?', '?', '?'])\n\n    return specific_h, general_h\n\n\n\n\n\n\n\ns_final, g_final = CE(concept,target)\n\nprint(\"Final Specific_h: \", s_final, sep=\"\\n\")\nprint(\"Final General_h: \", g_final, sep=\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-01-16T17:15:43.168070Z","iopub.execute_input":"2022-01-16T17:15:43.168723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ساحت داده تست ","metadata":{}},{"cell_type":"code","source":"X = finalData.drop(['result'],axis = 1)\ny = finalData.result.values\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4-  محاسبه الگوریتم بیز","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\n\n# Training the Naive Bayes model on the Training set\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nac = accuracy_score(y_test,y_pred)\ncm = confusion_matrix(y_test, y_pred)\n\nprint(\"NB accuracy score: \", ac )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5-  محاسبه الگوریتم knn\n","metadata":{}},{"cell_type":"markdown","source":"به دلیل زمان بر بودن کامنت شده است ","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors=3)\n#classifier.fit(X_train, y_train)\n\n##Predict the response for test dataset\n#y_pred = classifier.predict(X_test)\n\n##Import scikit-learn metrics module for accuracy calculation\n#from sklearn import metrics\n## Model Accuracy, how often is the classifier correct?\n#print(\"Knn Accuracy: \",metrics.accuracy_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6-  محاسبه الگوریتم کلاسترینگ","metadata":{}},{"cell_type":"markdown","source":"چون ویژگی های ما بیشتر از 2 بعد هست نمی شود آن ها را روی نمودار نمایش داد \n","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=3).fit(finalData)\ncentroids = kmeans.cluster_centers_\nprint(centroids)\n\n# plt.scatter(finalData['x'], finalData['y'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)\n# plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7-  محاسبه الگوریتم درخت تصمیم تصادفی","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n\n# Create Decision Tree classifer object\nclf = DecisionTreeClassifier()\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,y_train)\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"DecisionTree accuracy: \",metrics.accuracy_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8-  محاسبه الگوریتم درخت تصمیم ID3","metadata":{}},{"cell_type":"code","source":"import math\nfrom collections import deque\n\nclass Node:\n    \"\"\"Contains the information of the node and another nodes of the Decision Tree.\"\"\"\n\n    def __init__(self):\n        self.value = None\n        self.next = None\n        self.childs = None\n\n\nclass DecisionTreeClassifier:\n    \"\"\"Decision Tree Classifier using ID3 algorithm.\"\"\"\n\n    def __init__(self, X, feature_names, labels):\n        self.X = X\n        self.feature_names = feature_names\n        self.labels = labels\n        self.labelCategories = list(set(labels))\n        self.labelCategoriesCount = [list(labels).count(x) for x in self.labelCategories]\n        self.node = None\n        self.entropy = self._get_entropy([x for x in range(len(self.labels))])  # calculates the initial entropy\n\n    def _get_entropy(self, x_ids):\n        \"\"\" Calculates the entropy.\n        Parameters\n        __________\n        :param x_ids: list, List containing the instances ID's\n        __________\n        :return: entropy: float, Entropy.\n        \"\"\"\n        # sorted labels by instance id\n        labels = [self.labels[i] for i in x_ids]\n        # count number of instances of each category\n        label_count = [labels.count(x) for x in self.labelCategories]\n        # calculate the entropy for each category and sum them\n        entropy = sum([-count / len(x_ids) * math.log(count / len(x_ids), 2) if count else 0 for count in label_count])\n        return entropy\n\n    def _get_information_gain(self, x_ids, feature_id):\n        \"\"\"Calculates the information gain for a given feature based on its entropy and the total entropy of the system.\n        Parameters\n        __________\n        :param x_ids: list, List containing the instances ID's\n        :param feature_id: int, feature ID\n        __________\n        :return: info_gain: float, the information gain for a given feature.\n        \"\"\"\n        # calculate total entropy\n        info_gain = self._get_entropy(x_ids)\n        # store in a list all the values of the chosen feature\n        x_features = [self.X[x][feature_id] for x in x_ids]\n        # get unique values\n        feature_vals = list(set(x_features))\n        # get frequency of each value\n        feature_vals_count = [x_features.count(x) for x in feature_vals]\n        # get the feature values ids\n        feature_vals_id = [\n            [x_ids[i]\n            for i, x in enumerate(x_features)\n            if x == y]\n            for y in feature_vals\n        ]\n\n        # compute the information gain with the chosen feature\n        info_gain = info_gain - sum([val_counts / len(x_ids) * self._get_entropy(val_ids)\n                                     for val_counts, val_ids in zip(feature_vals_count, feature_vals_id)])\n\n        return info_gain\n\n    def _get_feature_max_information_gain(self, x_ids, feature_ids):\n        \"\"\"Finds the attribute/feature that maximizes the information gain.\n        Parameters\n        __________\n        :param x_ids: list, List containing the samples ID's\n        :param feature_ids: list, List containing the feature ID's\n        __________\n        :returns: string and int, feature and feature id of the feature that maximizes the information gain\n        \"\"\"\n        # get the entropy for each feature\n        features_entropy = [self._get_information_gain(x_ids, feature_id) for feature_id in feature_ids]\n        # find the feature that maximises the information gain\n        max_id = feature_ids[features_entropy.index(max(features_entropy))]\n\n        return self.feature_names[max_id], max_id\n\n    def id3(self):\n        \"\"\"Initializes ID3 algorithm to build a Decision Tree Classifier.\n        :return: None\n        \"\"\"\n        x_ids = [x for x in range(len(self.X))]\n        feature_ids = [x for x in range(len(self.feature_names))]\n        self.node = self._id3_recv(x_ids, feature_ids, self.node)\n        print('')\n\n    def _id3_recv(self, x_ids, feature_ids, node):\n        \"\"\"ID3 algorithm. It is called recursively until some criteria is met.\n        Parameters\n        __________\n        :param x_ids: list, list containing the samples ID's\n        :param feature_ids: list, List containing the feature ID's\n        :param node: object, An instance of the class Nodes\n        __________\n        :returns: An instance of the class Node containing all the information of the nodes in the Decision Tree\n        \"\"\"\n        if not node:\n            node = Node()  # initialize nodes\n        # sorted labels by instance id\n        labels_in_features = [self.labels[x] for x in x_ids]\n        # if all the example have the same class (pure node), return node\n        if len(set(labels_in_features)) == 1:\n            node.value = self.labels[x_ids[0]]\n            return node\n        # if there are not more feature to compute, return node with the most probable class\n        if len(feature_ids) == 0:\n            node.value = max(set(labels_in_features), key=labels_in_features.count)  # compute mode\n            return node\n        # else...\n        # choose the feature that maximizes the information gain\n        best_feature_name, best_feature_id = self._get_feature_max_information_gain(x_ids, feature_ids)\n        node.value = best_feature_name\n        node.childs = []\n        # value of the chosen feature for each instance\n        feature_values = list(set([self.X[x][best_feature_id] for x in x_ids]))\n        # loop through all the values\n        for value in feature_values:\n            child = Node()\n            child.value = value  # add a branch from the node to each feature value in our feature\n            node.childs.append(child)  # append new child node to current node\n            child_x_ids = [x for x in x_ids if self.X[x][best_feature_id] == value]\n            if not child_x_ids:\n                child.next = max(set(labels_in_features), key=labels_in_features.count)\n                print('')\n            else:\n                if feature_ids and best_feature_id in feature_ids:\n                    to_remove = feature_ids.index(best_feature_id)\n                    feature_ids.pop(to_remove)\n                # recursively call the algorithm\n                child.next = self._id3_recv(child_x_ids, feature_ids, child.next)\n        return node\n\n    def printTree(self):\n        if not self.node:\n            return\n        nodes = deque()\n        nodes.append(self.node)\n        while len(nodes) > 0:\n            node = nodes.popleft()\n            print(node.value)\n            if node.childs:\n                for child in node.childs:\n                    print('({})'.format(child.value))\n                    nodes.append(child.next)\n            elif node.next:\n                print(node.next)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tree = DecisionTreeClassifier(X=concept, feature_names=finalData.columns, labels=target)\nprint(\"System entropy {:.8f}\".format(tree.entropy))\ntree.printTree()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}