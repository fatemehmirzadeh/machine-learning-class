{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-16T15:51:58.097255Z","iopub.execute_input":"2022-01-16T15:51:58.097548Z","iopub.status.idle":"2022-01-16T15:51:58.112413Z","shell.execute_reply.started":"2022-01-16T15:51:58.097516Z","shell.execute_reply":"2022-01-16T15:51:58.111667Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"<div dir=\"rtl\">\n    در این قسمت کارهای زیر را انجام  می دهیم\n    \n    * خواندن فایل از پایگاه داده\n    * اضافه کردن برچسب مثبت بودن نتیجه تست به داده ها\n    * حذف ستون های id و سن به دلیل نیاز نبودن و کامل نبودن\n    * حذف سطرهای تکراری از پایگاه داده\n</div>\n","metadata":{}},{"cell_type":"code","source":"# read in all our data\nX_data = pd.read_json(\"../input/covid-patient-datasets/covid.json\")\nX_data['result_test'] = 'yes'\n#X_data.info()\nto_drop=['#','age']\nX_data.drop(to_drop,inplace=True,axis=1)\n#print(X_data.shape)\nX_data.head()\nnum_iter=X_data.duplicated().sum()\nprint(f'{num_iter} of data are iterated')\nX_data=X_data.drop_duplicates()\ndisplay(X_data)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T15:52:01.828167Z","iopub.execute_input":"2022-01-16T15:52:01.828886Z","iopub.status.idle":"2022-01-16T15:52:01.913345Z","shell.execute_reply.started":"2022-01-16T15:52:01.828847Z","shell.execute_reply":"2022-01-16T15:52:01.912419Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"<div dir=\"rtl\">\n    در ادامه کارهای زیر را برای مهیا کردن داده‌ها انجام  می‌دهیم\n    \n    * عددی کردن پایگاه داده با تبدیل مقادیر yes به 1 و no به صفر. لازم به ذکر است به دلیل اینکه در داده‌های ثبت شده اشتباهاتی وجود داشته و مواردی نیز ناهمگونی دیده میشد مجبور به تغییر موارد دیگری نیز شدیم.\n    * اضافه کردن مجموعه داده‌های دیگر به صورتی که شرایط عکس داده‌های فعلی را داشته و برچسب نتیجه تست آن نیز منفی باشد.\n    * ادغام دو مجموعه و ایجاد دیتاست نهایی\n    * نرمال کردن داده با استفاده از کتابخانه sklearn\n</div>","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\n\nX_data.replace(('yes','es', 'Yes', 'no', 'No' , ''), (1, 1, 1, 0, 0 , 0), inplace=True)\nY_data = X_data.copy()\nY_data.replace((1, 0), (0, 1), inplace=True)\n\ndata = X_data.append(Y_data, ignore_index=True)\n# Randomize the dataset\ndata = data.sample(frac=1, random_state=1)\nfeature_vector = X_data.columns\ndata_normalized = preprocessing.normalize(data)\n#display(data)\n#display(X_data)\n#display(Y_data)\ndisplay(data_normalized)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T15:52:05.043600Z","iopub.execute_input":"2022-01-16T15:52:05.044529Z","iopub.status.idle":"2022-01-16T15:52:05.067030Z","shell.execute_reply.started":"2022-01-16T15:52:05.044488Z","shell.execute_reply":"2022-01-16T15:52:05.066462Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"**FIND-S Algorithm**","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nd = np.array(data_normalized)[:,:-1]\ntarget = np.array(data_normalized)[:,-1]\n#display(d)\n#display(target)\n# traing function to implement Find-s algorithm\ndef find_s(c,t):\n    for i, val in enumerate(t):\n        if val == 1:\n            specific_hypothesis = c[i].copy()\n            break\n             \n    for i, val in enumerate(c):\n        if t[i] == 1:\n            for x in range(len(specific_hypothesis)):\n                if val[x] != specific_hypothesis[x]:\n                    specific_hypothesis[x] = -1\n                else:\n                    pass\n                 \n    return specific_hypothesis\n\n\nprint(\"The final hypothesis is:\",find_s(d,target))","metadata":{"execution":{"iopub.status.busy":"2022-01-16T15:52:08.723370Z","iopub.execute_input":"2022-01-16T15:52:08.723801Z","iopub.status.idle":"2022-01-16T15:52:08.736898Z","shell.execute_reply.started":"2022-01-16T15:52:08.723769Z","shell.execute_reply":"2022-01-16T15:52:08.736089Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**Candidate Elimination algorithm**","metadata":{}},{"cell_type":"code","source":"d = np.array(data_normalized)[:,:-1]\ntarget = np.array(data_normalized)[:,-1]\n\n#Candidate Elimination algorithm\ndef candidate_elimination(concepts, target):\n    specific_h = concepts[0].copy()\n    #print(\"Initialization of specific_h and general_h\")\n    #print(\"specific_h: \",specific_h)\n    general_h = [[-1 for i in range(len(specific_h))] for i in range(len(specific_h))]\n    #print(\"general_h: \",general_h)\n    #print(\"concepts: \",concepts)\n    for i, h in enumerate(concepts):\n        if target[i] == 1:\n            for x in range(len(specific_h)):\n                if h[x] != specific_h[x]:\n                    specific_h[x] = -1\n                    general_h[x][x] = -1\n        if target[i] == 0:\n            for x in range(len(specific_h)):\n                if h[x] != specific_h[x]:\n                    general_h[x][x] = specific_h[x]\n                else:\n                    general_h[x][x] = -1\n    #print(\"\\nSteps of Candidate Elimination Algorithm: \",i+1)\n    #print(\"Specific_h: \",i+1)\n    #print(specific_h,\"\\n\")\n    #print(\"general_h :\", i+1)\n    #print(general_h)\n    indices = [i for i, val in enumerate(general_h) if val == [-1, -1, -1, -1, -1, -1]]\n    #print(\"\\nIndices\",indices)\n    for i in indices:\n        general_h.remove([-1, -1, -1, -1, -1, -1])\n    return specific_h, general_h\n\ns_final,g_final = candidate_elimination(d, target)\nprint(\"\\nFinal Specific_h:\", s_final, sep=\"\\n\")\nprint(\"Final General_h:\", g_final, sep=\"\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2022-01-16T15:52:12.819277Z","iopub.execute_input":"2022-01-16T15:52:12.819835Z","iopub.status.idle":"2022-01-16T15:52:12.835001Z","shell.execute_reply.started":"2022-01-16T15:52:12.819799Z","shell.execute_reply":"2022-01-16T15:52:12.834407Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"**Split Train and Test data**","metadata":{}},{"cell_type":"code","source":"d = np.array(data)[:,:-1]\ntarget = np.array(data)[:,-1]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(d, target, test_size = 0.30, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T15:52:16.784164Z","iopub.execute_input":"2022-01-16T15:52:16.784531Z","iopub.status.idle":"2022-01-16T15:52:16.790788Z","shell.execute_reply.started":"2022-01-16T15:52:16.784503Z","shell.execute_reply":"2022-01-16T15:52:16.789820Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"**Naive Bayes Algorithm**","metadata":{}},{"cell_type":"code","source":"# Naive Bayes Algorithm\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_temp = sc.fit_transform(X_train)\nX_test_temp = sc.transform(X_test)\n\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train_temp, Y_train)\n\ny_pred  =  classifier.predict(X_test_temp)\n\nfrom sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(Y_test, y_pred))\nprint(classification_report(Y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-01-16T15:55:04.051189Z","iopub.execute_input":"2022-01-16T15:55:04.051830Z","iopub.status.idle":"2022-01-16T15:55:04.064930Z","shell.execute_reply.started":"2022-01-16T15:55:04.051800Z","shell.execute_reply":"2022-01-16T15:55:04.064317Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"**KNN**","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors=5)\nclassifier.fit(X_train, Y_train)\ny_pred1 = classifier.predict(X_test)\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(Y_test, y_pred1))\nprint(classification_report(Y_test, y_pred1))","metadata":{"execution":{"iopub.status.busy":"2022-01-16T15:53:19.552345Z","iopub.execute_input":"2022-01-16T15:53:19.552602Z","iopub.status.idle":"2022-01-16T15:53:19.728207Z","shell.execute_reply.started":"2022-01-16T15:53:19.552574Z","shell.execute_reply":"2022-01-16T15:53:19.727191Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"**Decision Tree**","metadata":{}},{"cell_type":"code","source":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\n# Create Decision Tree classifer object\nclf = DecisionTreeClassifier()\n\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,Y_train)\n\n#Predict the response for test dataset\ny_pred3 = clf.predict(X_test)\nfrom sklearn.metrics import classification_report, confusion_matrix\n# Model Accuracy, how often is the classifier correct?\nprint(confusion_matrix(Y_test, y_pred3))\nprint(classification_report(Y_test, y_pred3))","metadata":{"execution":{"iopub.status.busy":"2022-01-16T16:41:49.633734Z","iopub.execute_input":"2022-01-16T16:41:49.634036Z","iopub.status.idle":"2022-01-16T16:41:49.647478Z","shell.execute_reply.started":"2022-01-16T16:41:49.634000Z","shell.execute_reply":"2022-01-16T16:41:49.646606Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"from sklearn import tree\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X_train,Y_train)\nimg=tree.plot_tree(clf)\n\n[...]","metadata":{"execution":{"iopub.status.busy":"2022-01-16T16:49:40.990726Z","iopub.execute_input":"2022-01-16T16:49:40.990986Z","iopub.status.idle":"2022-01-16T16:49:43.334057Z","shell.execute_reply.started":"2022-01-16T16:49:40.990958Z","shell.execute_reply":"2022-01-16T16:49:43.333209Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"**Decision Tree ID3 Algorithm**","metadata":{}},{"cell_type":"code","source":"import math\nfrom collections import deque\n\nclass Node:\n    \"\"\"Contains the information of the node and another nodes of the Decision Tree.\"\"\"\n    def __init__(self):\n        self.value = None\n        self.next = None\n        self.childs = None\n\n\nclass DecisionTreeClassifier:\n    \"\"\"Decision Tree Classifier using ID3 algorithm.\"\"\"\n\n    def __init__(self, X, feature_names, labels):\n        self.X = X\n        self.feature_names = feature_names\n        self.labels = labels\n        self.labelCategories = list(set(labels))\n        self.labelCategoriesCount = [list(labels).count(x) for x in self.labelCategories]\n        self.node = None\n        self.entropy = self._get_entropy([x for x in range(len(self.labels))])  # calculates the initial entropy\n        return\n\n    def _get_entropy(self, x_ids):\n        \"\"\" Calculates the entropy.\n        Parameters\n        __________\n        :param x_ids: list, List containing the instances ID's\n        __________\n        :return: entropy: float, Entropy.\n        \"\"\"\n        # sorted labels by instance id\n        labels = [self.labels[i] for i in x_ids]\n        # count number of instances of each category\n        label_count = [labels.count(x) for x in self.labelCategories]\n        # calculate the entropy for each category and sum them\n        entropy = sum([-count / len(x_ids) * math.log(count / len(x_ids), 2) if count else 0 for count in label_count])\n        return entropy\n\n    def _get_information_gain(self, x_ids, feature_id):\n        \"\"\"Calculates the information gain for a given feature based on its entropy and the total entropy of the system.\n        Parameters\n        __________\n        :param x_ids: list, List containing the instances ID's\n        :param feature_id: int, feature ID\n        __________\n        :return: info_gain: float, the information gain for a given feature.\n        \"\"\"\n        # calculate total entropy\n        info_gain = self._get_entropy(x_ids)\n        # store in a list all the values of the chosen feature\n        x_features = [self.X[x][feature_id] for x in x_ids]\n        # get unique values\n        feature_vals = list(set(x_features))\n        # get frequency of each value\n        feature_vals_count = [x_features.count(x) for x in feature_vals]\n        # get the feature values ids\n        feature_vals_id = [\n            [x_ids[i]\n            for i, x in enumerate(x_features)\n            if x == y]\n            for y in feature_vals\n        ]\n\n        # compute the information gain with the chosen feature\n        info_gain = info_gain - sum([val_counts / len(x_ids) * self._get_entropy(val_ids)\n                                     for val_counts, val_ids in zip(feature_vals_count, feature_vals_id)])\n\n        return info_gain\n\n    def _get_feature_max_information_gain(self, x_ids, feature_ids):\n        \"\"\"Finds the attribute/feature that maximizes the information gain.\n        Parameters\n        __________\n        :param x_ids: list, List containing the samples ID's\n        :param feature_ids: list, List containing the feature ID's\n        __________\n        :returns: string and int, feature and feature id of the feature that maximizes the information gain\n        \"\"\"\n        # get the entropy for each feature\n        features_entropy = [self._get_information_gain(x_ids, feature_id) for feature_id in feature_ids]\n        # find the feature that maximises the information gain\n        max_id = feature_ids[features_entropy.index(max(features_entropy))]\n\n        return self.feature_names[max_id], max_id\n\n    def id3(self):\n        \"\"\"Initializes ID3 algorithm to build a Decision Tree Classifier.\n        :return: None\n        \"\"\"\n        x_ids = [x for x in range(len(self.X))]\n        feature_ids = [x for x in range(len(self.feature_names))]\n        self.node = self._id3_recv(x_ids, feature_ids, self.node)\n        print('')\n\n    def _id3_recv(self, x_ids, feature_ids, node):\n        \"\"\"ID3 algorithm. It is called recursively until some criteria is met.\n        Parameters\n        __________\n        :param x_ids: list, list containing the samples ID's\n        :param feature_ids: list, List containing the feature ID's\n        :param node: object, An instance of the class Nodes\n        __________\n        :returns: An instance of the class Node containing all the information of the nodes in the Decision Tree\n        \"\"\"\n        if not node:\n            node = Node()  # initialize nodes\n        # sorted labels by instance id\n        labels_in_features = [self.labels[x] for x in x_ids]\n        # if all the example have the same class (pure node), return node\n        if len(set(labels_in_features)) == 1:\n            node.value = self.labels[x_ids[0]]\n            return node\n        # if there are not more feature to compute, return node with the most probable class\n        if len(feature_ids) == 0:\n            node.value = max(set(labels_in_features), key=labels_in_features.count)  # compute mode\n            return node\n        # else...\n        # choose the feature that maximizes the information gain\n        best_feature_name, best_feature_id = self._get_feature_max_information_gain(x_ids, feature_ids)\n        node.value = best_feature_name\n        node.childs = []\n        # value of the chosen feature for each instance\n        feature_values = list(set([self.X[x][best_feature_id] for x in x_ids]))\n        # loop through all the values\n        for value in feature_values:\n            child = Node()\n            child.value = value  # add a branch from the node to each feature value in our feature\n            node.childs.append(child)  # append new child node to current node\n            child_x_ids = [x for x in x_ids if self.X[x][best_feature_id] == value]\n            if not child_x_ids:\n                child.next = max(set(labels_in_features), key=labels_in_features.count)\n                print('')\n            else:\n                if feature_ids and best_feature_id in feature_ids:\n                    to_remove = feature_ids.index(best_feature_id)\n                    feature_ids.pop(to_remove)\n                # recursively call the algorithm\n                child.next = self._id3_recv(child_x_ids, feature_ids, child.next)\n        return node\n\n    def printTree(self):\n        if not self.node:\n            return\n        nodes = deque()\n        nodes.append(self.node)\n        while len(nodes) > 0:\n            node = nodes.popleft()\n            print(node.value)\n            if node.childs:\n                for child in node.childs:\n                    print('({})'.format(child.value))\n                    nodes.append(child.next)\n            elif node.next:\n                print(node.next)\n\ntree_clf = DecisionTreeClassifier(X=X_train, feature_names=feature_vector[:-1], labels=Y_train)\nprint(\"System entropy {:.4f}\".format(tree_clf.entropy))","metadata":{"execution":{"iopub.status.busy":"2022-01-16T16:05:44.650893Z","iopub.execute_input":"2022-01-16T16:05:44.651198Z","iopub.status.idle":"2022-01-16T16:05:44.676993Z","shell.execute_reply.started":"2022-01-16T16:05:44.651167Z","shell.execute_reply":"2022-01-16T16:05:44.676157Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"**Find 5 worst features**","metadata":{}},{"cell_type":"code","source":"resultDicDT = {}\nresultDicNB = {}\n\nfor i in range(len(X_train[0])):\n    X_train_removed_feature = X_train\n    X_test_removed_feature = X_test\n    for row in X_train_removed_feature:\n        np.delete(row,i)\n    for row in X_test_removed_feature:\n        np.delete(row,i)\n  \n    # Decision Tree\n    # Create Decision Tree classifer object\n    clf = DecisionTreeClassifier()\n    # Train Decision Tree Classifer\n    clf = clf.fit(X_train_removed_feature ,Y_train)\n    #Predict the response for test dataset\n    y_pred3 = clf.predict(X_test_removed_feature)\n    # Model Accuracy, how often is the classifier correct?\n    ac = metrics.accuracy_score(Y_test, y_pred3)\n    resultDicDT[i] = ac\n\n    # Naive Bayes Algorithm\n    from sklearn.preprocessing import StandardScaler\n    sc = StandardScaler()\n    X_train_temp = sc.fit_transform(X_train_removed_feature)\n    X_test_temp = sc.transform(X_test_removed_feature)\n    from sklearn.naive_bayes import GaussianNB\n    classifier = GaussianNB()\n    classifier.fit(X_train_temp, Y_train)\n    y_pred  =  classifier.predict(X_test_temp)\n    from sklearn.metrics import confusion_matrix,accuracy_score\n    ac = accuracy_score(Y_test,y_pred)\n    resultDicNB[i] = ac\n\ndef translate(i):\n    for f, b in zip(data.columns,range(len(data.columns))):\n        if i == b:\n          return f\n\nprint(\"\\nDT:\")  \nsort_orders_DT = sorted(resultDicDT.items(), key=lambda x: x[1], reverse=True)\nfor i in sort_orders_DT:\n    print(\"After removing feature\", translate(i[0]) ,\"Accuracy is:\", i[1])\n\nprint(\"\\nNB:\")\nsort_orders_NB = sorted(resultDicNB.items(), key=lambda x: x[1], reverse=True)\nfor i in sort_orders_NB:\n    print(\"After removing feature\", translate(i[0]) ,\"Accuracy is:\", i[1])","metadata":{"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}