# -*- coding: utf-8 -*-
"""Final_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vWdomT3fZ_s8ZEF-Ihsp5S_qYBa9O6qo
"""

import numpy as np
import pandas as pd

from google.colab import drive
drive.mount('/drive')

data_set = pd.read_excel('/drive/MyDrive/Datasets/covid.xlsx')
data_set.drop(labels = ["#" ,"age"] ,axis = 1,inplace = True)
data_set.head()

def encode (x):
  if (x == "yes"):
    return 1

  return 0

columns = data_set.columns
for col in columns:
  data_set[col] = data_set[col].apply(encode)

data_set.head()

data_set.info()

data_set.drop_duplicates(inplace=True)
np.random.seed(1)
random_numbers = np.random.randint(0, 2, size = (500,21))
print(random_numbers)
temp_df = pd.DataFrame(random_numbers , columns = columns)
data_set ["covid"] = 1
temp_df ["covid"] = 0
df = pd.concat([data_set,temp_df] , axis=0)
df.drop_duplicates(subset=columns , keep="first",inplace=True)

from sklearn.model_selection import train_test_split
X = df[columns]
y = df["covid"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)

from sklearn.tree import DecisionTreeClassifier


d_tree = DecisionTreeClassifier(random_state=1,criterion = 'gini' , splitter = "random")
d_tree.fit(X_train,y_train)

d_tree_score = d_tree.score(X_test,y_test)

x = zip(columns , d_tree.feature_importances_)
x = list(x)
x.sort(key=lambda tup: tup[1])

print("deghate model random DecisionTreeClassifier : ",d_tree_score)
print("5 feature ba kamtarin ahamiat:")
x[0:6]

from sklearn.tree import DecisionTreeClassifier


d_tree2 = DecisionTreeClassifier(random_state=1,criterion = 'entropy' , splitter = "best")
d_tree2.fit(X_train,y_train)

d_tree2_score = d_tree2.score(X_test,y_test)

x = zip(columns , d_tree2.feature_importances_)
x = list(x)

print("deghate model random DecisionTreeClassifier ba id3: ",d_tree2_score)

"""find _ s"""

d = np.array(X_train,dtype=str)[:,:-1]
target = np.array(y_train,dtype=str)

# traing function to implement Find-s algorithm
def find_s(c,t):
    for i, val in enumerate(t):
        if val == "1":
            specific_hypothesis = c[i].copy()
            break
             
    for i, val in enumerate(c):
        if t[i] == "1":
            for x in range(len(specific_hypothesis)):
                if val[x] != specific_hypothesis[x]:
                    specific_hypothesis[x] = "?"
                else:
                    pass
                 
    return specific_hypothesis


print("The final hypothesis is:",find_s(d,target))

"""CE"""

d = np.array(X_train)[:,:-1]
target = np.array(y_train)

#Candidate Elimination algorithm
def candidate_elimination(concepts, target):
    specific_h = concepts[0].copy()
    print("Initialization of specific_h and general_h")
    print("specific_h: ",specific_h)
    general_h = [[-1 for i in range(len(specific_h))] for i in range(len(specific_h))]
    print("general_h: ",general_h)
    print("concepts: ",concepts)
    for i, h in enumerate(concepts):
        if target[i] == 1:
            for x in range(len(specific_h)):
                if h[x] != specific_h[x]:
                    specific_h[x] = -1
                    general_h[x][x] = -1
        if target[i] == 0:
            for x in range(len(specific_h)):
                if h[x] != specific_h[x]:
                    general_h[x][x] = specific_h[x]
                else:
                    general_h[x][x] = -1
    print("\nSteps of Candidate Elimination Algorithm: ",i+1)
    print("Specific_h: ",i+1)
    print(specific_h,"\n")
    print("general_h :", i+1)
    print(general_h)
    indices = [i for i, val in enumerate(general_h) if val == [-1, -1, -1, -1, -1, -1]]
    print("\nIndices",indices)
    for i in indices:
        general_h.remove([-1, -1, -1, -1, -1, -1])
    return specific_h, general_h

s_final,g_final = candidate_elimination(d, target)
print("\nFinal Specific_h:", s_final, sep="\n")
print("Final General_h:", g_final, sep="\n")

"""bayse"""

from sklearn.naive_bayes import CategoricalNB

nb = CategoricalNB()
nb.fit(X_train,y_train)

x = nb.score(X_test,y_test)
print("deghate model  Categorical naive bayes : ",x)

"""KNeighborsClassifier"""

from sklearn.neighbors import KNeighborsClassifier

neigh = KNeighborsClassifier(n_neighbors=3)
neigh.fit(X_train,y_train)
x = neigh.score(X_test,y_test)
print("deghate model KNeighborsClassifiers ba k=3: ",x)

"""k means"""

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=2, random_state=0).fit(X_train)


y_true = y_test.to_numpy()

x = kmeans.predict(X_test)
z = 0
for i in range(0,len(X_test)):
  if x[i] == y_true[i]:
    z = z+1
z = z/len(X_test)
print("score model KMeans ba k=2: ",(1-z))

