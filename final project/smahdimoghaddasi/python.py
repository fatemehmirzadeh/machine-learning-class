# -*- coding: utf-8 -*-
"""python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z8a-q7UYL7jJxWY67V-RheTDbEfhedgR
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

covid = pd.read_excel('covid.xlsx');
covid

covid=covid.drop(['#'], axis=1);
covid

covid=covid.drop(['age'], axis=1);
covid

covid.describe()



covid.Diarrhea.unique()

covid=covid.replace({'yes':'Yes', 'no':'No'});
covid.describe()

covid.Abdominal_pain.unique()

covid=covid.replace({'es':'Yes'});

covid.Abdominal_pain.unique()

np.where(pd.isnull(covid))

covid=covid.drop(covid.index[396])
np.where(pd.isnull(covid))

covid=covid.replace({'Yes':1, 'No':0});
covid

covid=covid.drop_duplicates(keep=False)
covid.shape

covid.describe()

covid.hist(bins=15,figsize=(20,20))

plt.figure(figsize=(20,20))
cor = covid.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()

covid =covid.drop(['urticaria','Vomit','Body_discoloration' ,'Abdominal_pain','Sneezing'],axis=1)
covid

columns={}
for col in covid.columns:
    columns[col]=[1,0];
columns

from itertools import product
All_V=pd.DataFrame([row for row in product(*columns.values())],columns=columns.keys())

All_V

df4 = pd.concat([covid,All_V])
no_covid=df4.drop_duplicates(keep=False)
no_covid

no_covid=no_covid[(no_covid.sum(axis=1)<8)  & (no_covid.sum(axis=1)>3)];
no_covid

no_covid=no_covid.sample(n=250)
no_covid

no_covid['class']=0;
covid['class']=1;
no_covid

dataset =pd.concat([no_covid,covid],ignore_index=True)
dataset

X=dataset.iloc[:,:-1]
y =dataset.iloc[:,-1]
y

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 4)

dataset_nominal=dataset.replace({1:'Yes', 0:'No'});
dataset_nominal

h = ['0', '0', '0', '0', '0', '0','0','0','0','0','0','0','0','0','0','0','0','0','0','0']

for index, row in dataset.iterrows():
    if row[-1] == 'Yes':
        j = 0
        
        for col in row:
            if col != 'Yes':
                if col != h[j] and h[j] == '0':
                    h[j] = col
                elif col != h[j] and h[j] != '0':
                    h[j] = '?'
                    
            j = j + 1
 
    
print('Maximally Specific Hypothesis: ', h)

import numpy as np
import pandas as pd

def learn(concepts, target):
    
    specific_h = concepts[0].copy()
    
    general_h = [["?" for i in range(len(specific_h))] for i in range(len(specific_h))]
    
    # The learning iterations
    for i, h in enumerate(concepts):
        
        # Checking if the hypothesis has a positive target
        if target[i] == "Yes":
            for x in range(len(specific_h)):
                
                # Change values in S & G only if values change
                if h[x] != specific_h[x]:
                    specific_h[x] = '?'
                    general_h[x][x] = '?'
                    
        # Checking if the hypothesis has a positive target
        if target[i] == "No":
            for x in range(len(specific_h)):
                
                # For negative hyposthesis change values only  in G
                if h[x] != specific_h[x]:
                    general_h[x][x] = specific_h[x]
                else:
                    general_h[x][x] = '?'
    
    # find indices where we have empty rows, meaning those that are unchanged
    indices = [i for i,val in enumerate(general_h) if val == ['?', '?', '?', '?', '?', '?']]
    for i in indices:
        # remove those rows from general_h
        general_h.remove(['?', '?', '?', '?', '?', '?'])
        
    # Return final values
    return specific_h, general_h
sample=dataset.sample(n=30);
concepts =np.array(sample.iloc[:,:-1]);
target=np.array(sample.iloc[:,-1]);
s_final, g_final = learn(concepts, target)
print("Final S:", s_final, sep="\n")
print("Final G:", g_final, sep="\n")

from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
y_pred = gnb.fit(X_train, y_train).predict(X_test)
y_pred
gnb.score(X_test, y_test)

from sklearn.neighbors import KNeighborsClassifier
error_rate = []
for i in range(1,50):
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train,y_train)
    pred_i = knn.predict(X_test)
    error_rate.append(np.mean(pred_i != y_test))
plt.figure(figsize=(15,8))
plt.plot(range(1,50),error_rate,color='green', linestyle='dashed', marker='o',
 markerfacecolor='red', markersize=12)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')

neigh = KNeighborsClassifier(n_neighbors=5)
neigh.fit(X_train, y_train)

y_pred=neigh.predict(X_test)
y_pred
neigh.score(X_test, y_test)

from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier(random_state=0,criterion='entropy')
clf.fit(X_train, y_train)

y_pred=clf.predict(X_test)
y_pred
clf.score(X_test, y_test)

from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier(random_state=0,splitter='random')
clf.fit(X_train, y_train)

y_pred=clf.predict(X_test)
y_pred
clf.score(X_test, y_test)

from sklearn.cluster import KMeans

wcss = []
for i in range(1, 16):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)
    
#Plotting the results onto a line graph, allowing us to observe 'The elbow'
plt.plot(range(1, 16), wcss)
plt.title('The elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS') #within cluster sum of squares
plt.show()

from sklearn.metrics import accuracy_score
kmeans = KMeans(n_clusters=2, random_state=1).fit(X)
kmeans.labels_
print(accuracy_score(y,kmeans.labels_))

from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
models = []
models.append(("Logistic Regression:",LogisticRegression()))
models.append(("Naive Bayes:",GaussianNB()))
models.append(("K-Nearest Neighbour:",KNeighborsClassifier(n_neighbors=3)))
models.append(("Decision Tree:",DecisionTreeClassifier()))
models.append(("Support Vector Machine-linear:",SVC(kernel="linear")))
models.append(("Support Vector Machine-rbf:",SVC(kernel="rbf")))
models.append(("Random Forest:",RandomForestClassifier(n_estimators=7)))
models.append(("MLP:",MLPClassifier(hidden_layer_sizes=(45,30,15),solver='sgd',learning_rate_init=0.01,max_iter=500)))
models.append(("AdaBoostClassifier:",AdaBoostClassifier()))
models.append(("GradientBoostingClassifier:",GradientBoostingClassifier()))
print('Models appended...')
results = []
names = []
for name,model in models:
    kfold = KFold(n_splits=10,shuffle=True, random_state=0)
    cv_result = cross_val_score(model,X_train,y_train, cv = kfold,scoring = "accuracy")
    names.append(name)
    results.append(cv_result)
for i in range(len(names)):
    print(names[i],results[i].mean()*100)

dataset.to_excel("covid-out.xlsx",index=False)