22. مفاهیم زیر را به صورت خلاصه بررسی کنید.
  -	Ovefitting
  	
   مفهوم overfit یا بیش برازش یعنی اینکه یک فرضیه بیش از حد برروی داده های آزمایشی منطبق شده است.یا به عبارتی ما بیش از حد به داده های آموزشی اهمیت داده ایم.در overfit فرضیه ای بدست می آید که برروی داده های آموزشی بسیار خوب عمل می کند اما بر رو ی مثال های واقعی دقت آن کاهش می یابد.
   راه حل ساده ای برای تشخیص آن وجود ندارد.
   به عنوان مثال، شکل زیر را در نظر بگیرید.که در آن دو فرضیه h1,h2 برروی داده ها نمایش داده شده است.
   
   ![](https://github.com/semnan-university-ai/machine-learning-class/blob/main/excersiecs/HamidehEhsani/22/IMG_20211223_121245.jpg)
   
   که در آن error train (h1) < error train(h2) است ولی error test(h1) > error test(h2) است.
   همانطور که در شکل ملاحظه می شود، فرضیه h2 فرضیه ی بهتری است زیرا بر روی مثال های واقعی بهتر عمل کرده است. پس h1 برروی داده های آموزشی overfit شده است.
   
  -	Local minimum
  
 در الگوریتم ژنتیک مطرح می شود و به این معنی است که در این الگوریتم ممکن است طبق نتایج فکر کنیم که به جواب بهینه رسیده ایم اما ان جواب یک جواب بهینه ی محلی باشد و اگر الگوریتم را ادامه دهیم میتوانیم به جواب بهینه کلی دست پیدا کنیم.
 روش‌های کلاسیک ریاضیات دارای دو اشکال اساسی هستند. اغلب این روش‌ها نقطه ها بهینه محلی را به عنوان نقطه بهینه کلی در نظر می‌گیرند و نیز هر یک از این روش‌ها تنها برای مسئله خاصی کاربرد دارند. 
به شکل زیر توجه کنید. این منحنی دارای دو نقطه ماکزیمم می‌باشد؛ که یکی از آن‌ها تنها ماکزیمم محلی است. حال اگر از روش‌های بهینه سازی ریاضی استفاده کنیم مجبوریم تا در یک بازه بسیار کوچک مقدار ماکزیمم تابع را بیابیم؛ مثلاً از نقطه ۱ شروع کنیم و تابع را ماکزیمم کنیم. بدیهی است اگر از نقطه ۱ شروع کنیم تنها به مقدار ماکزیمم محلی دست خواهیم یافت و الگوریتم ما پس از آن متوقف خواهد شد. اما در روش‌های هوشمند، به ویژه الگوریتم ژنتیک به دلیل خصلت تصادفی آن‌ها حتی اگر هم از نقطه ۱ شروع کنیم باز ممکن است در میان راه نقطه A به صورت تصادفی انتخاب شود که در این صورت ما شانس دست‌یابی به نقطه بهینه کلی  را خواهیم داشت.
مشکل بهینه محلی در الگوریتم ژنتیک بسیار شایع است. در حالتی که الگوریتم در بهینه محلی قرار بگیرد، استفاده از عملگر جهش سبب می‌شود تا جواب‌های بدتری نسبت به بهینه محلی تولید شود. با این حال، برای فرار از دام بهینه محلی می‌توان از عملگر ترکیب استفاده کرد. البته، از آنجایی که جهش یک فرایند کاملا تصادفی است، این امکان نیز وجود دارد که جهش‌های بزرگی در کروموزوم‌ها ایجاد شوند و آن کروموزوم‌ها یا جواب‌های کاندید از بهینه محلی خارج شوند.

![](https://github.com/semnan-university-ai/machine-learning-class/blob/main/excersiecs/HamidehEhsani/22/local%20minimom.jpeg)


  -	Gradient descent
  -	Eager and lazy learning

روش های یادگیری از یک نظر به دو دسته ی eager و lazy تقسیم می شوند .
در یادگیری 2 فاز داریم: آموزش و تست.

در Eager وقتی مثال ها توسط الگوریتم دریافت می شود، سعی می کند از روی مثال های آموزشی یک فرضیه بدست آورد و در مورد مثال های جدید (تست) ،فرضیه تصمیم گیری می کند.مانند الگوریتم درخت تصمیم.
در فاز آموزش، این دسته سریعتر عمل می کند.
 در روش lazy تقریبا به مثال های آموزشی کاری نداریم و فقط وقتی از آن سوالی پرسیده شود وارد عمل می شود و سعی می کند از روی مثال ها به جواب برسد.پس این روش در یادگیری سریعتر است زیرا عملا هیج کاری در این مرحله انجام نمی دهد.
