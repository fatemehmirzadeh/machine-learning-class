
<div dir="rtl">

### قضیه بیز (Bayesian Theorem)
در نیمه دوم قرن ۱۸، هنوز شاخه‌ای از ریاضیات به نام آمار و احتمال بوجود نیامده بود. در نتیجه بیشتر قضیه‌ها و تئوری‌‌های احتمال توسط ریاضی‌دانان شناخته و اثبات می‌شد. به این ترتیب اصول و قضیه‌های احتمال را «الگوهای شانس«» (Doctrine of Chances) می‌نامیدند زیرا در کتابی که توسط دمویر (Abraham de Moievre) نوشته شده بود، او از این اصطلاح استفاده کرده بود. در مقاله‌ای با نام «روش‌های محاسبات در الگوهای شانسی» (An Essay towards solving a Problem in the Doctrine of Chances) که توسط بیز در سال 1763 نوشت و توسط دوستش ریچار پرایس (Richard Price) منتشر شد نیز به بررسی شیوه محاسبه احتمال برای پدیده‌های شانسی و تصادفی پرداخته شده است.

او در این مقاله که به نظر ساده می‌آمد، احتمال توام، احتمال شرطی و احتمال حاشیه‌ای را مطرح کرد و به کمک آن‌ها عکس قضیه احتمال شرطی را ارائه داد.

از آن به بعد بین دو گروه از پیروان مکتب «احتمال برمبنای فراوانی» (Probability Bases on Frequency) و  «احتمال برمبنای بیز» (Bayesian Probability) اختلاف نظر و درگیری‌های زیادی بوجود آمده است. ولی بهتر است به دور از این اختلاف نظرها به منطق و دستآورد قضیه بیز بپردازیم.

اگر A و B دو پیشامد از فضای نمونه باشند، آنگاه می‌توان احتمال A به شرط B را برحسب احتمال B نوشت. این رابطه در زیر دیده می‌شود.
 
 <br/>
 
 <div dir="ltr">
P(A|B) = P(B|A) P(A) / P(B)
  </div>

 <br/>
 به طرف راست این تساوی، احتمال پسین می‌گویند. همچنین قسمت اول صورت کسر نیز، تابع درستنمایی و قسمت آخر هم احتمال پیشین نامیده می‌شود.

این رابطه می‌تواند تصورات و نظر ما را در مورد احتمال رخداد (احتمال پیشین) یک پیشامد با استفاده از شواهدی که در دست داریم (تابع درستنمایی) بهبود بخشیده و مقدار احتمال جدیدی به نام احتمال پسین را ارائه دهد.

البته ممکن است شواهدی که توسط داده‌ها تهیه شده در جهت تایید یا خلاف احتمال پیشین باشند. ولی به هر حال انتظار است که اطلاعات اضافه حاصل شده از مشاهدات در دقت محاسبه احتمال آن پدیده شانسی موثر باشند. به این ترتیب به کمک این رابطه می‌توانید فرضیاتی که در رابطه به احتمال رخداد یک پدیده دارید را بهبود دهید.

در مباحث مربوط به آمار و احتمال، فرضیات همان اعتقادات ما در مورد طبیعت پدیده‌های شانسی هستند که ممکن است هرگز نیز موفق به دیدن آن‌ها نشویم. ولی می‌توانیم به کمک متغیرهای تصادفی (که می‌توانیم مقدار آن‌ها را البته با کمی خطا، اندازه‌گیری کنیم) حدسیاتی نسبتا دقیق در مورد پارامترهای (طبیعت) اتفاقات تصادفی (پدیده‌های شانسی) داشته باشیم.

معمولا در آمار برای متغیرهای تصادفی یک توزیع احتمالی در نظر گرفته می‌شود. ولی در مباحث یادگیری ماشین این توزیع احتمال را می‌توان مجموعه‌ای از قوانین (منطقی یا پردازش‌ها) در نظر گرفت که توسط مثال یا «داده‌های آموزش» (Training Data) قابل ایجاد و حتی به‌روزرسانی هستند تا نقاط مخفی و تاریک پدیده‌های شانسی را نمایان کنند.

با این شرح سعی می‌کنیم در ادامه قضیه بیز را به زبان یادگیری ماشین و «علم داده» (Data Science)، نمایش و توضیح دهیم. به این ترتیب در اینجا، مجموعه داده را با D و فرضیه را با h نمادی گذاری می‌کنیم. پس با این کار سعی داریم که رابطه شناخته شده بیز را برای تشخیص اینکه چه فرضیه (قانونی) در بین داده‌ها وجود دارد به کار بگیریم.

براساس این نمادها، دوباره رابطه بیز را می‌نویسیم: 
 <br/>
 
  <div dir="ltr">
 P(h|D) = P(D|h) P(h) / P(D)
  </div>
 
 <br/>
 
 به این ترتیب به نظر می‌رسد که فضای فرضیات، می‌تواند بی‌نهایت بزرگ یا تعداد فرضیات خیلی زیاد باشد. مزیت استفاده از «استنباط بیزی» (Bayesian Inference) در این است فرضیاتی را انتخاب می‌کند که بیشترین شباهت را با داده‌های مشاهده شده دارند. زیرا در دنیای احتمالات، چیزی که واقعا اتفاق افتاده و قابل اندازه‌گیری است همان مقدارهای متغیرهای تصادفی یعنی مشاهدات هستند.
 
 <br/><br/>
 
توماس بیز بنیان گذار نگاه بیزي در سال ١٧١٩ وارد دانشگاه ادینبرو شد که در رشته منطق و الهیات تحصیل کند. در بازگشت سال ١٧٢٢ در کنار پدر خود
در کلیساي کوچکی مشغول فعالیتشد. او همچنین ریاضی دان بود و در سال ١٧۴٠ کشف بدیعی را نمود که هرگز آن را منتشر نکرد، اما دوست وي ریچارد
پرایس آن را پس از مرگ وي در سال ١٧۶١ در میان یادداشت هاي او یافت، مجدداً ویرایش کرد و آن را منتشر نمود ولی تا زمان لاپلاس کسی بدان اهمیت
نداد تا اواخر قرن ١٨ میلادي که خصوصاً در اروپا داده ها از قابلیت اطمینان برابر برخوردار نبودند. پیر-سیمون لاپلاس، ریاضیدان جوان، به این باور رسید که
نظریه احتمال کلید را در دست دارد و او به طور مستقل مکانیسم بیز را کشفکرد و در سال ١٧٧۴ منتشر کرد. لاپلاس اصل را نه با یک معادله بلکه با کلمات
بیان کرد. امروزه آمار بیزي به عنوان رشته اي از علم آمار از لحاظ فلسفی و تعبیر احتمال بسیار پراهمیت است و به قضیه بیز که پس از مرگ بیز ارائه شد معروف
گشته است. آلن تورینگ دانشمند علوم کامپیوتر، ریاضیدان و فیلسوف بریتانیایی است که امروزه به عنوان پدر علم کامپیوتر و هوش مصنوعی شناخته می شود.
دستاوردهاي برجسته او در طول زندگی کوتاهش حاصل ماجراجویی هاي یک ذهن زیبا است که درنهایت با مرگی مشکوك براي همیشه خاموش شد. در طول
جنگ جهانی، تورینگ در بلچلی پارك مرکز کد شکنی انگلستان مشغول و براي مدتی مسئول بخش مربوط به تحلیل نوشته هاي رمزي نیروي دریایی آلمان
بود. او چند روش و به طور خاص از نگاه بیزي بدون اینکه نامش را ببرد براي شکستن رمزهاي آلمان ها ابداع کرد، همین طور روش ماشینی الکترومکانیکی که
می توانست ویژگی هاي ماشین اینیگما را پیدا کند نیز در زمره کارهاي بزرگ وي می توان قلمداد کرد. آلن تورینگ دانشمندي پیشرو بود که نقش مهمی در
توسعه علوم کامپیوتري و هوش مصنوعی و احیاي اندیشه بیزي ایفا کرد. تورینگ به کمک آزمایش تورینگ سهم مؤثر و محرکی در زمینه هوش مصنوعی
ارائه کرد. او سپس در آزمایشگاه ملی فیزیکدر انگلستان مشغول به کار شد و یکی از طرح هاي اولیه برنامه ذخیره شده کامپیوتر را ارائه کرد، هرچند که در
کار کند که به عنوان اولین کامپیوتر حقیقی دنیا شناخته شد. درهرحال بعدها « منچستر مارك ١ » واقع ساخته نشد. در ١٩۴٨ به دانشگاه منچستر رفت تا روي
نقش قاعده و قانون بیزي در تحولات علمی روزبه روز اهمیتآن آشکارتر می شود. بسیاري روش هاي احتمالی بیزي در قرن بیست و یک پیشرفت هاي مهمی را
در تبیین و به کارگیري آمار بیزي در توسعه علمی رقم زده است و معضلات زیادي از دنیا را حل و فصل کرده است تکنولوژي نوین جهانی در گرو اندیشه هاي
بیزي رشد کرده.

روشی برای دسته‌بندی پدیده‌ها، بر پایه احتمال وقوع یا عدم وقوع یک پدیده‌است و در نظریه احتمالات با اهمیت و پرکاربرد است. اگر برای فضای نمونه‌ای مفروضی بتوانیم چنان افرازی انتخاب کنیم که با دانستن اینکه کدامیک از پیشامدهای افراز شده رخ داده‌است، بخش مهمی از عدم قطعیت تقلیل می‌یابد.

این قضیه از آن جهت مفید است که می‌توان از طریق آن، احتمال یک پیشامد را با مشروط کردن نسبت به وقوع یا عدم وقوع یک پیشامد دیگر محاسبه کرد. در بسیاری از حالت‌ها، محاسبهٔ احتمال یک پیشامد به صورت مستقیم کاری دشوار است. با استفاده از این قضیه و مشروط کردن پیشامد مورد نظر نسبت به پیشامد دیگر، می‌توان احتمال مورد نظر را محاسبه کرد.

بدون شک قضیه بیز (Bayes Theorem) یکی از مهم‌ترین اصول آمار و احتمالات است که بخش قابل توجهی از دانش مدرن ما در حوزه‌های مختلف، به طور خاص در حوزه هوش مصنوعی و مهندسی کنترل، بر آن استوار است. پروازهای امن در خطوط هوایی، سیستم‌های نظارتی و کنترلی شبکه برق، روبات‌های متحرک، موتورهای جستجو و ده‌ها کاربرد دیگر در زندگی روزمره ما، بدون این قانون عملا نمی‌توانستند وجود داشته باشند. بخش قابل توجهی از دانش هوش مصنوعی، که وظیفه آن توسعه هوشمندی سیستم‌های کامپیوتری است، بر روی قانون بیز و آمار بیزی بنا شده است. به عقیده بسیاری از دانشمندان علوم کامپیوتر، اساسی‌ترین معادله توصیف کننده هوش، همین قانون بیز است.

اگر فضای نمونه ای توسط B1,B2,…,BnB1,B2,…,Bn افراز شده باشد، بطوری که P(Bi)>0 و P(Bi)>0 آنگاه برای هر پیشامد A می‌توانیم بنویسیم:

P(Bj|A)=P(Bj)P(A|Bj)/P(A)


 تحلیل بیزی ابزار تحلیلی قدرتمند برای مدلسازی آماری، تفسیر نتایج و پیش بینی داده هاست. از این رویکرد، می توان زمانی استفاده کرد که روش های استاندارد مبتنی بر فراوانی(کلاسیک) در دسترس نیستند و یا روش های کلاسیک موجود موفقیت چندانی ندارند. بهرحال قبل از استفاده از روش های بیزی باید از مزیت ها و معایب آن آگاهی داشته باشید. احتمالا جامعیت رویکرد بیزی مهم ترین مزیت روش شناسی آن نسبت به رویکرد کلاسیک است. استنباط های رویکرد بیزی بر اساس قاعده ساده بیز هستند که برای همه مدل های پارامتریک بکار می رود. همین مساله منجر به جامعیت و نیز سهولت کاربرد و تفسیر آن می شود. بهرحال رویکرد کلاسیک وابسته به روش های برآورد مختلف است که برای مسائل و مدل های آماری خاص طراحی شده اند. اغلب روش های استنباطی که برای طبقه خاصی از مسائل طراحی شده نمی تواند برای مدلهای دیگر نیز بکار رود.

در تحلیل بیزی می توانیم از اطلاعات قبلی که ممکن است بر اساس شواهد آزمایشی و یا باور شخصی بدست آیند استفاده کنیم تا نتایج دقیق تری بدست آوریم. برای مثال، وارد کردن اطلاعات قبلی می تواند اثرات منفی اندازه کم نمونه را کاهش دهد. مهم تر از آن این است که اطلاعات پیشین می تواند از چارچوب نظری و مفهومی بدست آید

با بکارگیری همه اطلاعات توزیع پسین برای پارامترهای مدل، استنباط آماری در این رویکرد نسبت به رویکرد کلاسیک جامع تر و انعطاف پذیرتر است.
استنباط بیزی  به دلیل استفاده  از توزیع پسین در مرحله برآورد و پیش بینی، دقت بالایی دارد که توزیع پسین یا به روش تحلیلی یا به روش عددی محاسبه می شود. این در حالی است که در بسیاری از روش های برآورد کلاسیک برای مثال حداکثر درست نمایی بر اساس فرض نرمال بودن مجانبی استنباط انجام می گیرد. استنباط بیزی یک تفسیر شهودی و  واضح تری از نتایج بر اساس احتمالات فراهم می کند. برای مثال فاصله های منطقی به صورت فاصله هایی که پارامترها یک احتمال معین را می گیرند تفسیر می شوند در حالی که فاصله اطمینان در رویکرد کلاسیک تفسیر واضح و آن چنان مستقیمی ندارد و یا حداقل به اندازه رویکرد بیزی سرراست نیست.
مدل های بیزی اصل درست نمایی(Berger and Wolpert 1988))  را برآورده می کنند. بر اساس این اصل اطلاعات در یک نمونه به صورت کامل توسط تابع درست نمایی نمایش داده می شود. لازم تحقق این قاعده این است که اگر تابع درست نمایی در یک مدل متناسب با تابع درست نمایی مدل دیگر است این دو مدل باید نتایج مشابهی داشته باشند. برخی از محققان ذکر کرده اند که روش های رویکرد کلاسیک ممکن است  اصل درست نمایی  را نقض کنند. به عنوان نکته آخر می توان گفت که دقت برآورد در تحلیل بیزی به اندازه نمونه محدود نیست.

با وجود مزیت های مفهومی و روش شناسی رویکرد بیزی کاربرد آن در عمل هنوز مورد بحث و مناقشه است. دو دلیل اصلی می توان برای آن ذکر کرد: 1- امکان تصریح اطلاعات پیشین به صورت ذهنی 2- چالش های محاسباتی در استفاده از روش های بیزی. مشکل اول زمینه ساز این  امکان می شود که افراد متفاوت توزیع های پیشین متفاوتی تصریح کنند. به همین دلیل طرفداران رویکرد کلاسیک معتقدند که روش های بیزی فاقد منطق و واقع بینی بوده و نباید استفاده شوند.


 روش بیز روش جالب و دقیقی است که بر اساس محاسباتریاضی و احتمال به دست می آید.
 در این روش از توضیحات مسئله استفاده میکنیم که خود برگرفته از تعداد زیادی نمونه مثال است و در احتمالات آن جای گرفته اند.
 روش های آماری نسبت به نویز حساسیت کمی دارند چون تاثیر تعداد کمی خطا در نمونه های زیاد آماری کم است.
 برخلاف درخت تصمیم با اضافه شدن نمونه مثال جدید ساختار تصمیم گیری خیلی بهم نمیریزد.
 دانش به صورت تدریجی اضافه میشود.
 اگر فقط یک ویژگی وجود داشته باشد یا ویژگی ها از هم مستقل باشند بسیار خوب عمل میکند اما اگر بیش از یک ویژگی داشته باشیم در صورتیکه میزان تاثیرشان را روی یکدیگر بدانیم می تواند به حل مساله کمک کنند، اگر تاثیر ویژگی ها بر روی یکدیگر را نداند با مشکل مواجه میشود.
 یکی از مشکلات روش بیز این است که به تعداد مثال های بسیار زیادی برای آمار گرفتن نیاز دارد.

 منابع: 
 
</div>
 
