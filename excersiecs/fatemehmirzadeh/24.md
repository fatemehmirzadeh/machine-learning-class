 #### 24)  تاریخچه پیدایش تئوری بیز را بررسی کنید و مزایا و معایب این تئوری را بررسی کنید.
 
 در نیمه دوم قرن ۱۸، هنوز شاخه‌ای از ریاضیات به نام آمار و احتمال بوجود نیامده بود. در نتیجه بیشتر قضیه‌ها و تئوری‌‌های احتمال توسط ریاضی‌دانان شناخته و اثبات می‌شد. به این ترتیب اصول و قضیه‌های احتمال را «الگوهای شانس«» (Doctrine of Chances) می‌نامیدند زیرا در کتابی که توسط دمویر (Abraham de Moievre) نوشته شده بود، او از این اصطلاح استفاده کرده بود. در مقاله‌ای با نام «روش‌های محاسبات در الگوهای شانسی» (An Essay towards solving a Problem in the Doctrine of Chances) که توسط بیز در سال 1763 نوشت و توسط دوستش ریچار پرایس (Richard Price) منتشر شد نیز به بررسی شیوه محاسبه احتمال برای پدیده‌های شانسی و تصادفی پرداخته شده است.

او در این مقاله که به نظر ساده می‌آمد، احتمال توام، احتمال شرطی و احتمال حاشیه‌ای را مطرح کرد و به کمک آن‌ها عکس قضیه احتمال شرطی را ارائه داد.

از آن به بعد بین دو گروه از پیروان مکتب «احتمال برمبنای فراوانی» (Probability Bases on Frequency) و  «احتمال برمبنای بیز» (Bayesian Probability) اختلاف نظر و درگیری‌های زیادی بوجود آمده است

قضیه بیز (به انگلیسی: Bayes' theorem) روشی برای دسته‌بندی پدیده‌ها، بر پایه احتمال وقوع یا عدم وقوع یک پدیده‌است و در نظریه احتمالات با اهمیت و پرکاربرد است. اگر برای فضای نمونه‌ای مفروضی بتوانیم چنان افرازی انتخاب کنیم که با دانستن اینکه کدامیک از پیشامدهای افراز شده رخ داده‌است، بخش مهمی از عدم قطعیت تقلیل می‌یابد.

این قضیه از آن جهت مفید است که می‌توان از طریق آن، احتمال یک پیشامد را با مشروط کردن نسبت به وقوع یا عدم وقوع یک پیشامد دیگر محاسبه کرد. در بسیاری از حالت‌ها، محاسبهٔ احتمال یک پیشامد به صورت مستقیم کاری دشوار است. با استفاده از این قضیه و مشروط کردن پیشامد مورد نظر نسبت به پیشامد دیگر، می‌توان احتمال مورد نظر را محاسبه کرد.

این رابطه به خاطر بزرگداشت توماس بیز فیلسوف انگلیسی به نام فرمول بیز معروف است.
<br>
قضیه بیز (نام این قضیه به افتخار دانشمند انگلیسی آمار «توماس بیز» (Thomas Bayes) که در سال‌ 1763 مقاله‌ای با این موضوع منتشر کرد، انتخاب شده است.) روشی برای دسته‌بندی پدیده‌ها، بر پایه احتمال وقوع یا عدم وقوع یک پدیده‌است و در نظریه احتمالات با اهمیت و پرکاربرد است. اگر برای فضای نمونه‌ای مفروضی بتوانیم چنان افرازی انتخاب کنیم که با دانستن اینکه کدامیک از پیشامدهای افراز شده رخ داده‌است، بخش مهمی از عدم قطعیت تقلیل می‌یابد.

این قضیه از آن جهت مفید است که می‌توان از طریق آن، احتمال یک پیشامد را با مشروط کردن نسبت به وقوع یا عدم وقوع یک پیشامد دیگر محاسبه کرد. در بسیاری از حالت‌ها، محاسبهٔ احتمال یک پیشامد به صورت مستقیم کاری دشوار است. با استفاده از این قضیه و مشروط کردن پیشامد مورد نظر نسبت به پیشامد دیگر، می‌توان احتمال مورد نظر را محاسبه کرد.

بدون شک قضیه بیز (Bayes Theorem) یکی از مهم‌ترین اصول آمار و احتمالات است که بخش قابل توجهی از دانش مدرن ما در حوزه‌های مختلف، به طور خاص در حوزه هوش مصنوعی و مهندسی کنترل، بر آن استوار است. پروازهای امن در خطوط هوایی، سیستم‌های نظارتی و کنترلی شبکه برق، روبات‌های متحرک، موتورهای جستجو و ده‌ها کاربرد دیگر در زندگی روزمره ما، بدون این قانون عملا نمی‌توانستند وجود داشته باشند. بخش
قابل توجهی از دانش هوش مصنوعی، که وظیفه آن توسعه هوشمندی سیستم‌های کامپیوتری است، بر روی قانون بیز و آمار بیزی بنا شده است. به عقیده بسیاری از دانشمندان علوم کامپیوتر، اساسی‌ترین معادله توصیف کننده هوش، همین قانون بیز است.

اگر فضای نمونه ای توسط B1,B2,…,BnB1,B2,…,Bn افراز شده باشد، بطوری که P(Bi)>0 و P(Bi)>0 آنگاه برای هر پیشامد A می‌توانیم بنویسیم:<br>
<br>
<div dir="ltr">

P(Bj|A)=P(Bj)P(A|Bj)/P(A)
  
 <br>
  <div dir="rtl">
   فرض می‌کنیم قضیه بیز یک افراز برای فضای نمونه‌ای قضیه بیز تشکیل دهند. طوری که به ازای هر قضیه بیز، داشته باشیم قضیه بیز و فرض کنید قضیه بیز پیشامدی با فرض قضیه بیز باشد، در اینصورت به ازای قضیه بیز، داریم:
    <br>
    ![image](https://user-images.githubusercontent.com/94124607/147201664-27343b33-192f-4cb3-bc34-67d79bc15929.png)<br>
 
   تئوری بیز در یادگیری ماشین <br>
    در یادگیری ماشین معمولاً در فضای فرضیه H بدنبال بهترین فرضیه‌ای هستیم که درمورد داده‌های آموزشی D صدق کند. یک راه تعیین بهترین فرضیه، این است که بدنبال محتمل ترین فرضیه‌ای باشیم که با داشتن داده‌های آموزشی D و احتمال قبلی در مورد فرضیه‌های مختلف می‌توان انتظار داشت تئوری بیز چنین راه حلی را ارائه می‌دهد. این روش راه حل مستقیمی است که نیازی به جستجو ندارد.

سنگ بنای یادگیری بیزی را تئوری بیز تشکیل می‌دهد. این تئوری امکان محاسبه احتمال ثانویه را بر مبنای احتمالات اولیه می‌دهد:
    <br>
    ![image](https://user-images.githubusercontent.com/94124607/147201826-e6051577-fbed-4d0e-84f1-ac2c4d9233fe.png)
<br>
  ####مزایا و معایب
    <br>
    تحقیقاتی در سال ۲۰۰۴ دلایل نظریه‌ای برای رفتارهای غیر منطقی بیز مطرح کرد و همچنین در سال ۲۰۰۶ مشاهدات فراگیری به منظور مقایسه این روش با سایر روش‌های طبقه‌بندی مانند boosted trees و جنگل تصادفی (random forests) انجام شد که بر کارا بودن این روش صحه گذاشتند.

از مزایای این روش می‌توان به موارد زیر اشاره کرد:

دسته‌بندی کردن داده‌های آزمایشی آسان و سریع است. همچنین زمانی که تعداد دسته‌ها از دو بیشتر باشد نیز عملکرد خوبی از خودش نشان می‌دهد.
تا زمانی که شرط مستقل بودن برقرار باشد، یک دسته‌بندی‌کننده بیز ساده عملکرد بهتری نسبت به مدل‌های دیگر مانند رگرسیون لجستیک دارد و به حجم آموزش کمی نیاز دارد.
در حالتی که ورودی‌هایمان دسته‌بندی شده باشند این روش عملکرد بهتری نسبت به حالی دارد که ورودی‌هایمان عدد باشند. برای حالتی که ورودی عدد باشد معمولاً فرض می‌شود که از توزیع نرمال پیروی می‌کنند. (که فرض قوی‌ای است)
علاوه بر مزایایی که این دسته‌بندی‌کننده دارد معایبی نیز دارد، از جمله:

در صورتی که ورودی‌مان دسته‌بندی شده باشد و در مرحلهٔ یادگیری دسته‌ای وجود داشته باشد که دسته‌بندی‌کننده هیچ داده‌ای از آن دسته مشاهده نکرده باشد، دسته‌بندی‌کننده احتمالی برابر صفر برای آن دسته در نظر می‌گیرد و قادر به دسته‌بندی کردن نخواهد بود. برای حل این مشکل می‌توان از تکنیک‌های هموارسازی مانند تخمین‌گر لاپلاس استفاده کرد.
یکی دیگر از معایب این دسته‌بندی‌کننده این است که دستیابی به شرط مستقل بودن در دنیای واقعی تقریباً غیرممکن است.
