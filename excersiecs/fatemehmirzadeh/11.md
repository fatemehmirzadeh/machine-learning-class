**سوال 11**<br>
<br>
**مراحل الگوریتم  ID3**<br>
<br>
در درخت تصمیم (ID3 )از یک مقدار آماري به نام بهره اطلاعات
 Gain Information استفاده میشود تا مشخص کنیم که یک صفت تا چه حد
قادر است مثالهاي آموزشی را بر حسب دسته بندي آنها جدا کند.<br>
 @گزینه اصلی در الگوریتم ID3 انتخاب صفت براي آزمون در هر گره از درخت است.<br>
 @تمایل داریم صفتی را انتخاب کنیم که براي دسته بندي مثالها مناسب ترین باشد.<br>
 @در این الگوریتم درخت تصمیم از بالا به پایین ساخته میشود.<br>
@این الگوریتم با این سوال شروع میشود: کدام صفت باید در ریشه درخت مورد
نظر قرار گیرد؟
<br>
<br>
براي پاسخ دادن به این سوال از یک آزمون آماري به نام بهره اطلاعاتی
(Gain Information )استفاده میشود تا مشخص شود هر صفت تا چه حد
قادر است به تنهایی مثالهاي موجود را دسته بندي کند.
<br>

  با انتخاب یک صفت، براي هر یک از مقادیر ممکن براي آن یک شاخه ایجاد
شده و مثالهاي آموزشی بر اساس صفت هر شاخه مرتب میشوند.<br>
@ سپس عملیات فوق براي مثالهاي قرار گرفته در هر شاخه تکرار میشوند تا
بهترین صفت براي گره بعدي انتخاب شود.<br>
@ این الگوریتم یک جستجوي حریصانه است که در آن انتخاب هاي قبلی هرگز
مورد بازبینی قرار نمیگیرند.<br>
@یک گره ریشه براي درخت ایجاد کن <br>
@ اگر همه مثالها مثبت هستند، درختی با گرهاي به نام گره ریشه، با برچسب (+)
برگردان.<br>
@ اگر همه مثالها منفی هستند، درختی با گرهاي به نام گره ریشه، با برچسب (-)
برگردان.<br>
@ اگر ویژگیها تهی هستند، درختی با گرهاي به نام گره ریشه، با برچسبی معادل متداول
ترین مقدار صفت – هدف در مثالها برگردان.<br>

**در غیر این صورت:**<br>
<div>
<dir="rtl">
 @آ صفت از صفاتی که به بهترین نحو مثالها را دسته بندي می کند در A قرار بده
  <br>
  @صفت تصمیم براي ریشه A
  <br>
  @ براي هر مقدار ممکن Vi از A
  <br>
  مطابق با آزمون Vi=A انشعابی جدید به درخت زیر ریشه اضافه کن
  <br>
  @ Vi Examples را به عنوان زیر مجموعه اي از مثال ها که داراي مقدار Vi براي Aاست در
نظر بگیر<br>
  **اگر Vi Examples تهی است**<br>
  @آنگاه زیر این انشعاب جدید، یک گره برگ با برچسبی معادل متداول ترین مقدار
صفت -هدف در مثال ها قرار بده<br>
  در غیراین صورت <br>
  در زیر این انشعاب جدید، زیر درخت زیر را اضافه کن<br>
 به طور کلی تر :<br>
 <div>
<dir ="ltr">
@Calculate entropy for dataset.<br>
@For each attribute/feature.<br>
@ Calculate entropy for all its categorical values.<br>
@ Calculate information gain for the feature.<br>
@Find the feature with maximum information gain.<br>
@Repeat it until we get the desired tree.<br>
 </div>
  <br>
  **مثال**
  <br>
![id31](https://user-images.githubusercontent.com/94124607/145092526-7680a1c2-b533-4603-b1f9-894be26f991d.png)<br>
  <div>
  <dir="ltr">
  Entropy(S) = -pcinema log2(pcinema) -ptennis log2(ptennis) -pshopping log2(pshopping) -pstay_in log2(pstay_in)<br>
                   = -(6/10) * log2(6/10) -(2/10) * log2(2/10) -(1/10) * log2(1/10) -(1/10) * log2(1/10)<br>
                   = -(6/10) * -0.737 -(2/10) * -2.322 -(1/10) * -3.322 -(1/10) * -3.322<br>
                   = 0.4422 + 0.4644 + 0.3322 + 0.3322 = 1.571<br>
   <br>
   Gain(S, weather) = 1.571 - (|Ssun|/10)*Entropy(Ssun) - (|Swind|/10)*Entropy(Swind) - (|Srain|/10)*Entropy(Srain)<br>
                          = 1.571 - (0.3)*Entropy(Ssun) - (0.4)*Entropy(Swind) - (0.3)*Entropy(Srain)<br>
                          = 1.571 - (0.3)*(0.918) - (0.4)*(0.81125) - (0.3)*(0.918) = 0.70<br>
<br>
   <br>
Gain(S, parents) = 1.571 - (|Syes|/10)*Entropy(Syes) - (|Sno|/10)*Entropy(Sno)<br>
                          = 1.571 - (0.5) * 0 - (0.5) * 1.922 = 1.571 - 0.961 = 0.61<br>
<br>
   <br>
Gain(S, money) = 1.571 - (|Srich|/10)*Entropy(Srich) - (|Spoor|/10)*Entropy(Spoor)<br>
                          = 1.571 - (0.7) * (1.842) - (0.3) * 0 = 1.571 - 1.2894 = 0.2816<br>
   <br>


  
  
  
   



