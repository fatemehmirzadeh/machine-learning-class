<div dir="rtl">
سوال: مفاهیم زیر را به صورت خلاصه بررسی کنید.
<br/>  
Ovefitting
<br/>
Local minimum
<br/>
Gradient descent
<br/>
Eager and lazy learning
</div>
<br/>
<div dir="rtl">  
<br/>  
Ovefitting
<br/>
داده های پرت. یعنی داده هایی که غلط نیستند اما احتمال وقوعشان خیلی کم است و تاثیرشان مثل نویز است.
هر چه درخت کوتاه تر باشد overfit کمتر رخ می دهد اما درخت کوتاه دقتش پایین می آید
<div/>
<br/>
<div dir="rtl"> 
 Local minimum
<br/>
فرض کنید در یک پاساژ مشغولِ خریدِ یک نوعِ خاص کفش هستید و در فروشگاههای مختلف، به دنبال 
فروشگاهی میگردید که ارزانترین قیمت را برای آن کفش داشته بدهد. یکی یکی فروشگاهها را سرکشی 
میکنید تا متوجه میشوید، مغازههای طبقهی ۴ معموالً اجناس را ارزانتر میفروشند. پس فروشگاههای طبقهی 
۴ را با دقتِ بیشتری میگردید تا باالخره به فروشگاهی میرسید که ارزانترین قیمت را در طبقهی ۴ در آن 
پاساژ میدهد. در واقع شما به یک حالتِ بهینه دست پیدا کردید ولی این بهینگی مخصوصِ آن پاساژ بوده است. 
شاید اگر پاساژهای دیگر را میگشتید، قیمتی پایینتر و بهینهتر نیز پیدا میکردید. میتوان گفت شما در یک 
.بهینهی محلی افتادهاید و ممکن است در پاساژهای دیگر، قیمتهای بهینهتری نیز موجود باشد
در دنیای الگوریتمها نیز، وضع به همین صورت است. فرض کنید یک نرمافزاری مانند اینستاگرام میخواهد به
شما یک دوستِ جدید برای دنبال کردن پیشنهاد دهد. اینستاگرام به دنبالِ این است که بهترین شخصِ مناسبِ 
شما را بهتان پیشنهاد دهد. در واقع به دنبال بهینهی سراسری )بهترین شخص در کل اینستاگرام برای معرفی به 
شما( است. اما پیدا کردنِ بهترین شخص، نیاز به جستجوی تمامیِ اشخاص در اینستاگرام دارد که طبیعتاً این 
که برایِ پیدا کردن بهترین خانه در کل شهر،. پس به جای آن اینستاگرام کار در زمانِ معقولی میسر نیست
سعی میکند تا جایی که میتواند و در یک زمانِ معقول به دنبال شخصی که مناسبِ شما باشد بگردد و این کار 
باعث میشود بتواند یک جواب بهینهی محلی را پیدا کرده و به عنوان یک پیشنهادِ معقول به شما نمایش دهد
<div/>
<br/>
<div dir="rtl">  
Gradient descent
<br/>
یک الگوریتم بهینهسازی کارآمد است. این الگوریتم تالش میکند تا مینیمم سراسریِ یک تابع اتالف را بیابد. این الگوریتم گرادیان 
محلیِ تابع خطا را نسبت به پارامتر تتا محاسبه نموده و در جهت کاهش گرادیان حرکت میکند. زمانی که این گرادیان صفر شود، 
یعنی خطا صفر است و تابع هزینه به مینیمم مقدار خود رسیده است.
<br/>
در بسیاری از کاربردها، اگر تابع هزینه بر حسب پارامتر تتا رسم شود، نمودار حاصل، یک منحنی محدب خواهد بود که تنها یک 
مینیمم دارد. شیب در نقطه مینیمم، دقیقا صفر است. این مینیمم، دقیقا نقطهای است که تابع هزینه باید به آن همگرا شود
<br/>
![pic1](https://github.com/semnan-university-ai/machine-learning-class/blob/main/excersiecs/Saedganjeey/22/1.png)
<br/>
در صورتی که مقداردهی اولیه به گونه ای باشد که نقطه اولیه در سمت چپ منحنی قرار گیرد، الگوریتم به
یک مینیمم محلی ختم خواهد شد. اما اگر مقداردهی اولیه به گونه ای صورت گیرد که نقطه اولیه در سمت
راست منحنی قرار گیرد، الگوریتم به نقطه هدف، یعنی مینیمم سراسری ختم خواهد شد. بنابراین مقداردهی
اولیه در این الگوریتم بسیار حائز اهمیت است.
<div/>
<br/>
<div dir="rtl">  
Eager and lazy learning
<br/>
 اکثر الگوریتمهای طبقهبندی از روش یادگیری کوشا استفاده میکنند. یادگیری کوشا به این صورت عمل 
میکند که با استفاده از دادههای آموزشی مدل طبقهبندی را میسازد، سپس این مدل، برای ارزیابی دادههای 
آزمایشی مورد استفاده قرار میگیرد. اگر نتایج ارزیابی رضایتبخش باشد، از مدل بدستآمده جهت پیشبینی 
طبقهبندی دادههای ناشناخته ورودی استفاده میشود.از این رو یادگیری کوشا پیش از این، بیشتر کار خود را در 
تدوین مدل انجام دادهاند.
از طرف دیگر یادگیری تنبل هیچ مدلی را قبل از گرفتن دادههای ناشناخته از ورودی نمیسازد و منتظر 
دادههای طبقهبندی نشده میماند و پس از دریافت شروع به ساخت مدل پیشبینی میکند.به دلیل اینکه به 
ازای هر پیشبینی داده ورودی باید کل مدل از اول ساخته شود، بنابراین میتوان گفت یاگیری تنبل زمان 
زیادی صرف میکند.
در متدهای تنبل گاهی تصمیمگیری برای چگونگی تعمیم بر روی دادههای آموزشی به نمونهی 
آموزشی ارائه شده نیز وابسته میشود.
 متدهای کوشا این وابستگی را نمیتوانند داشته باشند، زمانی که یک متد کوشا با یک نمونه جدید 
مواجه میشود، تخمین جهانی را انجام داده است.
 تفاوت این دو متد:
1 .تفاوت در زمان محاسبات
2 .تفاوت در طبقهبندهای تولید شده برای نمونههای جدید
یادگیری تنبل میتواند با ترکیب تخمینهای موضعی تابع هدف را یاد بگیرد، در حالی که یادگیر کوشا فقط یک 
تخمین جهانی را با توجه به نمونه آموزشی یاد میگیرد. این تفاوت بین یادگیری کوشا و تنبل به تفاوت بین 
تخمین موضعی و جهانی تابع هدف بر میگردد.
<div/>
<br/>