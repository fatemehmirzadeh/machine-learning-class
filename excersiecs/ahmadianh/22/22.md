<div dir="rtl">

## سوال 22

</div>

<div dir="rtl">

.مفاهیم زیر را به طور خلاصه بیان کنید

---
* **Overfiting:**

یعنی پیدا کردن فرضیه هایی که بر روی مثال های آموزشی خیلی خوب کار می کند اما در مثال های آزمایشی دقتش پایین  میاید

به عبارت دیگر به مثال های آموزشی خیلی بها می دهدو با اینکه دقت بالایی در مثال های آموزشی دارد اما دقتش در مثال های آزمایشی پایین میاید.

---
* **local minimum:**

 مینیم محلی یک تابع مختصاتی است که در آن تابع دارای کمترین مقدار نسبت به نقاط نزدیک خود باشد.


![minimum](https://raw.githubusercontent.com/semnan-university-ai/machine-learning-class/main/excersiecs/ahmadianh/22/images.png?token=AW2SJTTBW7ZKQJBNQZ5RF7TBYQKUI)

---

 * **Gradient descent**

 «گرادیان کاهشی» (Gradient Descent) یک الگوریتم بهینه‌سازی برای پیدا کردن کمینه یک تابع است. در این الگوریتم کار با یک نقطه تصادفی روی تابع آغاز می‌شود و روی جهت منفی از گرادیان تابع حرکت می‌کند تا به کمینه محلی/سراسری برسد

 ---

 * **Eager and lazy learning**
 
 الگوریتم‌های طبقه‌بندی (Classification) را  از نظر روش یادگیری روی داده‌های آموزشی، به ۲ گروه دسته‌بندی می کنند.

* (Lazy Learner)
* (Eager Learner)

اکثر الگوریتم‌های طبقه‌بندی از روش یادگیری eager استفاده می‌کنند. که به این صورت عمل می‌کند که با استفاده از داده‌های آموزشی(Train) مدل طبقه‌بندی را می‌سازد، سپس این مدل، برای ارزیابی داده‌های آزمایشی(Test) مورد استفاده قرار می‌گیرد. اگر نتایج ارزیابی رضایت‌بخش باشد، از مدل بدست‌آمده جهت پیش‌بینی طبقه‌بند(classifications) داده‌های ناشناخته ورودی استفاده می‌شود.

 یادگیری lazy  هیچ مدل از پیش تعیین نشده ندارد  و منتظر داده‌های طبقه‌بندی نشده می‌ماند و پس از دریافت شروع به ساخت مدل پیش‌بینی می‌کند.به دلیل اینکه به ازای هر پیش‌بینی داده ورودی باید کل مدل از اول ساخته شود، بنابراین می‌توان گفت یاگیری تنبل زمان زیادی صرف می‌کند.