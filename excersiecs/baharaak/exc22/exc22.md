## مفاهیم زیر را تعریف کنید:

### دoverfitting

د Overfit شدن به معنای این است که الگوریتم فقط داده‌هایی که در مجموعه آموزشی (train set) یاد گرفته است را می‌تواند به درستی پیش‌بینی کند ولی اگر داده‌ای کمی از مجموعه‌ی آموزشی فاصله داشته باشد، الگوریتمی که Overfit شده باشد، نمی‌تواند به درستی پاسخی برای این داده‌های جدید پیدا کند و آن‌ها را با اشتباهِ زیادی طبقه‌بندی می کند.

### دlocal minimum

د local minimum یا کمینه محلی: یک کمینه محلی x* تابع f عنصری است که که به ازای آن پارامتری مانند δ>0 وجود دارد، به طوریکه:

∀X∈A where |(|X-X^* |)|≤ δ

و به ازاء تمامی مقادیر X که قید (Constraint) بالا را ارضا می‌کنند، عبارت f(X*)<= f(X) برقرار است. به عبارت دیگر در برخی نواحی X*، تمام مقادیر تابع f، بزرگتر یا برابر با مقدار عنصر کمینه محلی هستند.

### دgradient descent

«گرادیان کاهشی» (Gradient Descent) یک الگوریتم بهینه‌سازی برای پیدا کردن کمینه یک تابع است. در این الگوریتم کار با یک نقطه تصادفی روی تابع آغاز می‌شود و روی جهت منفی از گرادیان تابع حرکت می‌کند تا به کمینه محلی/سراسری برسد.

### دegar and lazy descent


الگوریتم‌های طبقه‌بندی (Classification) را می‌توان از منظر روش یادگیری روی داده‌های آموزشی، به ۲ گروه زیر دسته‌بندی کرد.

یادگیری تنبل (Lazy Learner)
یادگیری کوشا (Eager Learner)

اکثر الگوریتم‌های طبقه‌بندی از روش یادگیری کوشا استفاده می‌کنند. یادگیری کوشا به این صورت عمل می‌کند که با استفاده از داده‌های آموزشی(Train) مدل طبقه‌بندی را می‌سازد، سپس این مدل، برای ارزیابی داده‌های آزمایشی(Test) مورد استفاده قرار می‌گیرد. اگر نتایج ارزیابی رضایت‌بخش باشد، از مدل بدست‌آمده جهت پیش‌بینی طبقه‌بند(classifications) داده‌های ناشناخته ورودی استفاده می‌شود.از این رو یادگیری کوشا پیش از این، بیشتر کار خود را در تدوین مدل انجام داده‌اند.

از طرف دیگر یادگیری تنبل هیچ مدلی را قبل از گرفتن داده‌های ناشناخته از ورودی نمی‌سازد و منتظر داده‌های طبقه‌بندی نشده می‌ماند و پس از دریافت شروع به ساخت مدل پیش‌بینی می‌کند.به دلیل اینکه به ازای هر پیش‌بینی داده ورودی باید کل مدل از اول ساخته شود، بنابراین می‌توان گفت یاگیری تنبل زمان زیادی صرف می‌کند.

