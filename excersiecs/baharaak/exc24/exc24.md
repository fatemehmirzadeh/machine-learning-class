### تاریخچه پیدایش تئوری بیز و مزایا و معایب

در نیمه دوم قرن ۱۸، هنوز شاخه‌ای از ریاضیات به نام آمار و احتمال بوجود نیامده بود. در نتیجه بیشتر قضیه‌ها و تئوری‌‌های احتمال توسط ریاضی‌دانان شناخته و اثبات می‌شد. به این ترتیب اصول و قضیه‌های احتمال را «الگوهای شانس«» (Doctrine of Chances) می‌نامیدند زیرا در کتابی که توسط دمویر (Abraham de Moievre) نوشته شده بود، او از این اصطلاح استفاده کرده بود. در مقاله‌ای با نام «روش‌های محاسبات در الگوهای شانسی» (An Essay towards solving a Problem in the Doctrine of Chances) که توسط بیز در سال 1763 نوشت و توسط دوستش ریچار پرایس (Richard Price) منتشر شد نیز به بررسی شیوه محاسبه احتمال برای پدیده‌های شانسی و تصادفی پرداخته شده است.

او در این مقاله که به نظر ساده می‌آمد، احتمال توام، احتمال شرطی و احتمال حاشیه‌ای را مطرح کرد و به کمک آن‌ها عکس قضیه احتمال شرطی را ارائه داد.

از آن به بعد بین دو گروه از پیروان مکتب «احتمال برمبنای فراوانی» (Probability Bases on Frequency) و  «احتمال برمبنای بیز» (Bayesian Probability) اختلاف نظر و درگیری‌های زیادی بوجود آمده است. ولی بهتر است به دور از این اختلاف نظرها به منطق و دستآورد قضیه بیز بپردازیم.

اگر A و B دو پیشامد از فضای نمونه باشند، آنگاه می‌توان احتمال A به شرط B را برحسب احتمال B نوشت. این رابطه در زیر دیده می‌شود.

p(A|B)=P(B|A)P(A)/P(B)

به طرف راست این تساوی، احتمال پسین می‌گویند. همچنین قسمت اول صورت کسر نیز، تابع درستنمایی و قسمت آخر هم احتمال پیشین نامیده می‌شود.

این رابطه می‌تواند تصورات و نظر ما را در مورد احتمال رخداد (احتمال پیشین) یک پیشامد با استفاده از شواهدی که در دست داریم (تابع درستنمایی) بهبود بخشیده و مقدار احتمال جدیدی به نام احتمال پسین را ارائه دهد.

البته ممکن است شواهدی که توسط داده‌ها تهیه شده در جهت تایید یا خلاف احتمال پیشین باشند. ولی به هر حال انتظار است که اطلاعات اضافه حاصل شده از مشاهدات در دقت محاسبه احتمال آن پدیده شانسی موثر باشند. به این ترتیب به کمک این رابطه می‌توانید فرضیاتی که در رابطه به احتمال رخداد یک پدیده دارید را بهبود دهید.

در مباحث مربوط به آمار و احتمال، فرضیات همان اعتقادات ما در مورد طبیعت پدیده‌های شانسی هستند که ممکن است هرگز نیز موفق به دیدن آن‌ها نشویم. ولی می‌توانیم به کمک متغیرهای تصادفی (که می‌توانیم مقدار آن‌ها را البته با کمی خطا، اندازه‌گیری کنیم) حدسیاتی نسبتا دقیق در مورد پارامترهای (طبیعت) اتفاقات تصادفی (پدیده‌های شانسی) داشته باشیم.

معمولا در آمار برای متغیرهای تصادفی یک توزیع احتمالی در نظر گرفته می‌شود. ولی در مباحث یادگیری ماشین این توزیع احتمال را می‌توان مجموعه‌ای از قوانین (منطقی یا پردازش‌ها) در نظر گرفت که توسط مثال یا «داده‌های آموزش» (Training Data) قابل ایجاد و حتی به‌روزرسانی هستند تا نقاط مخفی و تاریک پدیده‌های شانسی را نمایان کنند.

### مزایا 

1- فایده اول آنکه دسته بندی کردن داده های آزمایشی آسان و سریع است. همچنین زمانی که تعداد دسته ها از دو بیشتر باشد نیز عملکرد خوبی از خودش نشان می دهد.

2-فایده دوم آنکه تا زمانی که شرط مستقل بودن برقرار باشد، یک دسته بندی کننده بیز ساده عملکرد بهتری نسبت به مدل های دیگر مانند رگرسیون لجستیک دارد و به حجم آموزش کمی نیاز دارد.

3-فایده سوم آنکه در حالتی که ورودی هایمان دسته بندی شده باشند این روش عملکرد بهتری نسبت به حالی دارد که ورودی هایمان عدد باشند. برای حالتی که ورودی عدد باشد به طور معمول این طور فرض می شود که از توزیع نرمال پیروی می کنند. (که این خود می تواند فرض قوی ای به حساب بیاید)

### معایب

1- عیب اول آنکه در صورتی که ورودی مان دسته بندی شده باشد و در مرحله یادگیری دسته ای وجود داشته باشد که دسته بندی کننده هیچ داده ای از آن دسته مشاهده نکرده باشد، دسته بندی کننده احتمالی برابر صفر برای آن دسته در نظر می گیرد و قادر به دسته بندی کردن نخواهد بود. برای حل این مشکل می توان از تکنیک های هموارسازی مانند تخمین گر لاپلاس استفاده کرد.

2- عیب دیگر این الگوریتم آن است که دستیابی به شرط مستقل بودن در دنیای واقعی تقریبا غیر ممکن است.
