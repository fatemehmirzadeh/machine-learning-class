<div dir="rtl">
  
  ### مفاهیم زیر را به صورت خلاصه بررسی کنید.
  
  <br/>
  Ovefitting
  <br/>
  به شکل زیر توجه کنید:<br/>
  
  ![overfit](https://github.com/semnan-university-ai/machine-learning-class/blob/main/excersiecs/Eveaskari/Exc%20(22)/overfit.JPG)
  
  <br/>
  خط h1  فرضیه ایست که روی مثال های آموزشی ما خوب جواب می دهد. حال فرض کنید دو مثال نویز مانند در بین داده ها می باشند.(+ و – سبز رنگ). فرضیه h1  دیگر روی این مثال ها خوب عمل نمیکند. ولی فرضیه h2  که یک منحنی است روی تمام مثال های آموزشی ما جواب می دهد. ولی اگر یک مثال تست داشته باشیم و نزدیک به مثبت بین منفی ها و بالعکس باشد ممکن است درست جواب ندهد.  ولی فرضیه h1  بهتر عمل می کند ، چون ساده تر است و منطق بهتری دارد. در این حالت می گوییم فرضیه h2  روی مثال های آموزشی overfit  شده است. یعنی روی مثال آموزشی خوب جواب می دهد ولی ممکن است  روی مثال های تست دچار خطای بالا شود.
  <br/>
  error train (h2) < error train (h1)  <br/>
error D (h2) > error D (h1)

  <br/>
  و برای ما خطای روی داده آزمایشی مهم است(D) نه خطای روی داده آموزشی (train)
 D= کل توزیع(کل نمونه ها)

  <br/>
  در فرضیه ما خطرناک ترین قسمت نزدیک به مرز هاست زیرا قدرت تفکیک پایین است.
  <br/>
  <br/>
  Local minimum
  <br/>
  
  <br/>
  Gradient descent
  <br/>
  
  <br/>
  Eager and lazy learning
  
  <br/>
  
  
  <br/>
  
  </div>
