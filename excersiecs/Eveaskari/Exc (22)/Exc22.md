<div dir="rtl">
  
  ### مفاهیم زیر را به صورت خلاصه بررسی کنید.
  
  <br/>
  
  #### Ovefitting
  
  <br/>
  
  به شکل زیر توجه کنید:<br/>
  
  ![overfit](https://github.com/semnan-university-ai/machine-learning-class/blob/main/excersiecs/Eveaskari/Exc%20(22)/overfit.JPG)
  
  <br/>
  خط h1  فرضیه ایست که روی مثال های آموزشی ما خوب جواب می دهد. حال فرض کنید دو مثال نویز مانند در بین داده ها می باشند.(+ و – سبز رنگ). فرضیه h1  دیگر روی این مثال ها خوب عمل نمیکند. ولی فرضیه h2  که یک منحنی است روی تمام مثال های آموزشی ما جواب می دهد. ولی اگر یک مثال تست داشته باشیم و نزدیک به مثبت بین منفی ها و بالعکس باشد ممکن است درست جواب ندهد.  ولی فرضیه h1  بهتر عمل می کند ، چون ساده تر است و منطق بهتری دارد. در این حالت می گوییم فرضیه h2  روی مثال های آموزشی overfit  شده است. یعنی روی مثال آموزشی خوب جواب می دهد ولی ممکن است  روی مثال های تست دچار خطای بالا شود.
  <br/>
  error train (h2) < error train (h1)  <br/>
error D (h2) > error D (h1)

  <br/>
  و برای ما خطای روی داده آزمایشی مهم است(D) نه خطای روی داده آموزشی (train)
 D= کل توزیع(کل نمونه ها)

  <br/>
  در فرضیه ما خطرناک ترین قسمت نزدیک به مرز هاست زیرا قدرت تفکیک پایین است.
  <br/>
  <br/>
  
  #### Local minimum
  
  <br/>
  در شبکه های عصبی چندین شیوه آموزش داریم: 1. شیوه ترتیبی که نمونه ها تک تک برای اصلاح وزن ها به کار میروند. 2.شیوه دسته ای که تمام نمونه های آموزشی اعمال شده، سپس اصلاح وزن ها صورت میپذرد.
  <br/>
  شيوه ي ترتيبي به حافظه ي كمتري احتياج دارد.در صورتي كه نمونه ها به صورت ترتيبي و تصادفي اعمال شود، احتمال اين كه الگوريتم در دام مينيمم محلي بيفتند، كم تر خواهد بود.
  <br/>
  وقتي كه داده هاي تكراري داشته باشيم، روش ترتيبي به صورت مؤثرتري از داده هاي تكراري استفاده م يكند.هر چند اين تصادفي بودن، تحليل نظري شرايط همگرايي را دشوارتر م يكند، در حالي كه با استفاده از شيوه ي دسته اي تقريب بهتري از بردار گراديان به دست مي آيد و همگرايي به سوي مينيمم محلي تضمين شده است.
  <br/>
  استفاده از پردازش موازي در شيوه ي دسته اي به مراتب ساده است.مينيمم محلي جايي است كه مشتق صفر گردد
  <br/>
  
  ![localminima](https://github.com/semnan-university-ai/machine-learning-class/blob/main/excersiecs/Eveaskari/Exc%20(22)/localminima.JPG)
  
  
  <br/>
  
  #### Gradient descent
  
  <br/>
  برای درک الگوریتم شیب نزول ،فضای فرضیه ای و رابطه ی آن را با مقادیر E  تصور میکنیم.
  به شکل زیر نگاه کنید:
  <br/>
  
  ![gradient desent](https://github.com/semnan-university-ai/machine-learning-class/blob/main/excersiecs/Eveaskari/Exc%20(22)/gradient.JPG)
  
  <br/>
  در شکل دو محور w0 و  w1  دو مقدار ممکن برای بردار وزن واحد خطی هستند. محور سوم E میزان خطای مربوط به دسته ای از نمونه های آموزشی خاص را نشان میدهد.
  <br/>
  سطح خطای نشان داده شده در شکل ارجحیت هر بردار وزن را در فضای فرضیه ها نشان میدهد.( بردارهایی ارجحیت دارند که خطای کمتری داشته باشند). با توجه به نحوه ی تعریف E ، برای واحد های خطی، سطح خطا همیشه سهمی وار است و یک نقطه ی مینیمم مطلق خواهد داشت . این نقطه ی مینیمم مطلق، به دسته نمونه های آموزشی وابسته است.
  <br/>
  جستجوی شیب نزول بردار وزنی را مشخص میکند که در آن E کمینه است. در این الگوریتم   ابتدا از برداری دلخواه شروع کرده و مرحله به مرحله آن با تغییرهای کوچک به بردار وزن مطلوب میل می کند. در هر مرحله، بردار وزن به طرف بیشترین کاهش خطا حرکت داده می شود. این فراین آنقدر ادامه پیدا خواهد کرد تا به مینیمم مطلق تابع خطا برسیم.
  <br/>
  
  ![grad-equa](https://github.com/semnan-university-ai/machine-learning-class/blob/main/excersiecs/Eveaskari/Exc%20(22)/Grad-equa.JPG)
  
  <br/>
  <br/>
  
  #### Eager and lazy learning
  
  <br/>
  روش های یادگیری: Eager : از روی مثال ها فرضیه می سازد. Leazy :  با مثال ها در ابتدا کاری نداردو صبر میکند تا زمانی که سوالی پرسیده شود، سپس به سراغ مثال ها رفته و همسایه ها را چک میکند و محاسبه می کند و جواب میدهد.
  <br/>
  دسته های یادگیری: global (عمومی) : فرضیه ای داریم که تمام داده های بعدی را با این فرضیه حل می کنیم یا میسنجیم. Local  (محلی) : فقط همسایه ها را مانند روش  k-nn بررسی میکند و جواب میدهد.
  <br/>
  روش ایگر در دسته ی عمومی قرار دارد و روش لیزی در دسته محلی.(یعنی مانند یکدیگر عمل میکنند.)
  
  <br/>
  مقایسه سرعت: در لیزی آموزش سریع است چون اصلا هیچ آموزشی ندارد و در ایگر برای نمونههای تست سریع تر عمل میکند چون از قبل برای حل یک فرضیه تولید کرده است.
  <br/>
  و برای ما سرعت در نمونه های تست مهم تر است پس روش ایگر بهتر است.
  <br/>
  
  <br/>
  
  </div>
