<div dir="rtl">
Ovefitting :
<br/>
Overfit شدن : فرض کنید دو فرضیه h1 و  h2 موجود است حال فرضیه ی h2 روی داده های آموزشی بسیار خوب و دقیق عمل می کند و خطای آن صفر است ولی روی داده های آزمایشی یا همان داده های واقعی ( که در تعریف آن به کل داده ها اشاره کرده است) به نظر می رسد دقت آن کمتر از h1 است . در این حالت به اصطلاح می گوئیم h2 روی داده های ما overfit شده است . ( در کل یعنی فرضیه ای را پیدا کردیم که اگرچه روی داده های آزمایشی بسیار خوب عمل میکند ولی روی داده های آزمایشی دقت پایین تری دارد و فرضیه دیگری هست که بهتر از آن عمل می کند. )
گویی خیلی سعی کردم خودم رو شبیه مثال های آموزشی کنم .
<br/>
تعریف آن ساده است ولی تشخیص آن بسیار سخت است و راه مشخصی برای آن وجود ندارد .
<br/>
می توان گفت در این حالت با مجموعه خاصی از داده ها بسیار زیاد مطابقت دارد، 
و بنابراین ممکن است نتواند داده های اضافی را تطبیق دهد یا مشاهدات آینده را به طور قابل اعتماد پیش بینی کند.
<br/>
<br/>
Local minimum :
<br/>
بهینه محلی نقطه ای است که در حالت کلی min نیست ولی در ناحیه ای که انتخاب شده حالت min را دارد.
<br/>
به عبارت بهتر در بین همسایه های آن اگر در نظر گرفته شود گویی min ماست ولی در حالت کلی چنین چیزی درست نیست.
<br/>
دقیقا مثل اینکه در K-NN باتوجه به k همسایه خود تصمیم گیری را انجام دهیم و تصمیم با در نظر گرفتن تمامی نمونه ها متفاوت باشد.
<br/> 
<br/>
Gradient descent :
<br/>
پایه ای برای الگوریتم هایی که جستجو در فضای پیوسته فرضیه ای را انجام می دهند است.
<br/>
Gradient descent در واقع الگوریتم بهینه سازی با تکرار است تا به کمک آن بیتوان minimum یک تابع را یافت .
<br/>
 در این الگوریتم کار با یک نقطه تصادفی روی تابع آغاز می‌شود و خلاف جهت گرادیان تابع یا approximate gradient حرکت می‌کند تا به کمینه محلی یا سراسری برسد.
<b/>
ایده این است که گام های مکرر را در جهت مخالف approximate gradient در نقطه ای مشخص برداریم
<br/>
<br/>
Eager and lazy learning :
<br/>
دو روش یادگیری به حساب می آیند .
<br/>
روش های Eager روش هایی هستند که وقتی با مثال های آموزشی رو به رو می شوند سعی می کنند از آن ها به یک فرضیه دست پیدا کنند.
<br/>
سپس مثال ها را کنار گذاشته و با آمدن نمونه جدید تصمیم گیری را با همان فرضیه انجام می دهند همانند درخت تصمیم.
<br/>
اما
<br/>
روش lazy محاسبات را تا دستخ بندی نمونه های جدید به نعویق می اندازد به عبارت بهتر با ورود مثال ها کاری نمی کنند ولی به محض آمدن
نمونه جدید سعی می کنند با کمک مثال ها به جواب برسند ولی بدون هیچ فرضیه ای.
<br/>
مشابه روش K-NN
<br/>
از آنجا که زمان مرحله test برایمان مهم است باید گفت Eager ها زمان تست کمتری نسبت به lazy ها دارند
<br/>
ولی اگر بخواهیم زمان مرحله آموزش را مقایسه کنیم زمان مرحله آموزش در lazy ها کمتر از Eager هاست . 
</div>
