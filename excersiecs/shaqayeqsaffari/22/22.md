<div dir="rtl">
  
  #### مفاهیم زیر را به صورت خلاصه بررسی کنید.
  
  <br/>
  

  #### Ovefitting

  <br/>
   هر گاه فرضیه ای نسبت به فرضیه دیگر، روی داده های اموزش دارای خطایی باشد که مقدار ان از مقدار خطای فرضیه روی داده های اموزشی از فرضیه دیگر کمتر باشد اما خطای تست ان فرضیه از خطای تست فرضیه دیگر بیشتر شود میگوییم ان فرضیه دچار overfit شده ایم.
  این عمل زمانی رخ میدهد که معمولا فرضیه بسیار به داده های اموزشی فیت شده باشد و دارای پیچیدگی بیشتری باشد.
بطور مثال فرضیه سبزرنگ دچار overfit شده است و داده هایی هم که در این ناحیه نزدیک مرز بیاید به دسته مخالف دسته بندی میکند که بطور کل مرز ها بیشتر مستعد خطا هستن.
  <br/>
  در زیر نیز قابل مشاهده است که برای فرضیه سبزرنگh2 خطای کل داده ها که همیشه مهم تر است بیشتر شده اما برای فرضیه مشکی رنگh1 خطای داده های اموزشی بیشتر یوده است.
    <br/>
  error train (h2) < error train (h1) 
 <br/>
error D (h2) > error D (h1)
  <br/>

<img width="202" alt="Screen Shot 2021-12-23 at 10 46 40 AM" src="https://user-images.githubusercontent.com/94288799/147216777-3c404e90-1a7c-40ae-8137-b392d72685d1.png">

  <br/>
  
  #### Local minimum
  
<br/>
مفهومی که هم در شبکه های عصبی و هم در مسایل هوش مصنوعی هست بهینه محلی میباشد.

بهینه محلی میتواند هم لوکال مینیمم باشد و هم ماکزیمم محلی.

زمانی که به مینیمم محلی میرسیم درواقع به نقطه ای رسیدیم که همسایه های انزدیک اطراف ان دارای مقدار بیشتری از ان نقطه هستند و ما نمیدانیم که این نقطه مینیمم سراسری است یا محلی.  
<br/>
البته در ادامه مسیر اگر پیش برویم به طور مثال با رفتن به سمت نقاطی که دارای مقدار تابع بیشتری از نقطه مینیمم محلی دارد پیش میرویم تا شاید نقطه بهتری یافت شود. در صورتی که نقاط بهتری یافت شود به این نقطه مینیمم محلی میگوییم چون متوجه میشویم نقطه دیگری که دارای مقدار تابع کمتری شده است ان مینیمم سراسری بوده و این نقطه را متوجه مینیمم محلی بودنش میشویم.
یکی از راه های فرار از بهینه محلی در الگوریتم ژنتیک استفاده از جهش است و از راه های فرار از مینیمم محلی در شبکه های عصبی استفاده از تابع نزول گرادیان کاهشی هست.
<br/>
  <img width="473" alt="Screen Shot 2021-12-23 at 10 59 24 AM" src="https://user-images.githubusercontent.com/94288799/147215182-e6092df8-325c-4d23-9194-55f74484d3a6.png">

  
  <br/>
  
  #### Gradient descent:
<br/>
الگوریتم گرادیان کاهشی، یک الگوریتم بهینه‌سازی تکراری مرتبه-اول هست که مینیموم محلی در یک تابع مشتق‌پذیر را پیدا می‌کند.
<br/>
. در این الگوریتم کار با یک نقطه تصادفی روی تابع آغاز می‌شود و روی جهت منفی از گرادیان تابع حرکت می‌کند تا به کمینه محلی/سراسری برسد.
<br/>
گرادیان کاهشی رایج ترین الگوریتم بهینه سازی در یادگیری ماشین و یادگیری عمیق به حساب می آید. این یک الگوریتم بهینه سازی مرتبه اول (first-order) است. این بدان معنی است که هنگام به روزرسانی های پارامترها فقط مشتق اول را در نظر می گیرد. در هر تکرار پارامترها را در جهت عکس گرادیان تابع هدفJ(w)  به روزرسانی می کنیم آن هم با در نظر گرفتن پارامترهایی که گرادیان با تند ترین شیب به بالا سوق می دهد. اندازه گامی که در هر تکرار برای رسیدن به کمینه محلی (local minimum) برمی داریم با توجه به نرخ یادگیری α تعیین می شود. بنابراین مسیر سراشیبی را دنبال می کنیم تا به کمینه محلی (local minimum) برسیم.
<br/>

##### نحوه کار گرادیان کاهشی
<br/>
حال نحوه کار گرادیان کاهشی (Gradient Descent) بر روی رگرسیون لجستیک را با توجه به فرمول بررسی کنیم.  
<br/>
نکات:<br/>
یک مقدار برای نرخ یادگیری انتخاب میشود α  نرخ یادگیری اندازه گام هر تکرار را معین می کند
<br/>
اگر α خیلی کوچک باشد، مدت زمان زیادی طول می کشد تا همگرایی صورت گیرد و از نظر محاسباتی گران تمام می شود.
<br/>
نرخ یادگیری تعیین میکند میزان بزرگی حرکت ما به پایین چقدر باشد
<br/>
مشتق نیز تعیین کننده جهت حرکت است
<br/>
تغییر مقدار تتا نیز به صورت پیوسته انجام میشود به صورت جزیی تا تابع هزینه مینیمم شود.

<br/>
این کار به طور مکررانجام میشود تا به نقطه مینیمم برسیم.
<br/>
  <img width="556" alt="Screen Shot 2021-12-23 at 11 39 36 AM" src="https://user-images.githubusercontent.com/94288799/147215883-76c34bfd-36e9-4630-bfe4-0bdb426c2b91.png">


  <br/>
حال اگر فرض کنیم مینیمم تابعی که داریم به شکل زیر باشدارتفاع را که همان تابع هزینه هست میخواهیم مینیمم کنیم برای این کار با شروع از تتا ۰ و تتا ۱ که ابتدا مقادیر فرضی است شروع میکینم که همان نقطه شروع است.
  <br/>
به ازای هر متغیری برای تتا صفر و تتا یک مقدار مشخص برای تابع به دست خواهید آورد.
اگر بالای تپه بایستید و به اطراف نگاه کنید، می‌بینید که بهترین مسیر برای پایین رفتن این مسیر است: همین کار را تا انتها انجام داده تا به مینیمم سراسری برسیم.
  
 <br/> در این الگوریتم نیز با تعیین پارامترهای گفته شده در فرمول با پیروی قدم به قدم به سمت شیب نزولی میتوان به بهینه سراسری رسید
 <br/>
اگر از نقاط متفاوتی شروع کنیم ممکن است به نقاط متفارتی برسد 
 <br/>
  <img width="446" alt="Screen Shot 2021-12-23 at 11 41 27 AM" src="https://user-images.githubusercontent.com/94288799/147215950-619580e7-fc88-47ac-88c1-83260f7f9ce9.png">

 <br/> 
  
  #### Eager and lazy learning

   <br/>
روش های یادگیری میتوانند دسته lazy باشند یا دسته eager
  <br/>
در روش eager ابتدا از روی نمونه اموزشی یک فرضیه ایجاد میکند و داده ها را دیگر استفاده نمیکند تا هرگاه نمونه جدید بیاید بر اساس ان تصمیم گیری برای نمونه جدید انجام شود. 
اما در روش lazy از قبل فرضیه ای از روی نمونه های اموزشی ایجاد نمیشود ولی هر زمان داده جدید بیاید تمام محاسبات انجام میشود تا برچسب داده جدید مشخص شود یکی ز این مدل ها روش knn است.
  <br/>
روش lazy زمان تست بالایی دارد اما روش eager زمان ترین بالایی دارد .
  <br/>
روش lazy جزو روش های local هستند اما روش های eager جز روش های global هستند.

  <br/>
  <br/>
  </div>





