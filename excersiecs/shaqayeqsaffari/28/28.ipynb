{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled36.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaEskNr0lGTl",
        "outputId": "b326a224-1f52-4f03-adfb-55105bdc9e7a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/bayes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I51Srw-bdy9x",
        "outputId": "600628f3-3f02-400b-d5e2-a70a7ba0dea5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/bayes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "ihH3kzGnd7AD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c84f9ow6eAQp",
        "outputId": "7cd35cf5-6df2-4016-b8b9-fc08fae21059"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "**در پوشه input فایل پیامکی که میخواهید اسپم یا هم(غیر اسپم) بودنش را بفهمید (تست) قرار دهید. **\n",
        "</div>"
      ],
      "metadata": {
        "id": "AP7hxodZAU8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "نمونه تست غیر اسپم: سلام مواظب خانواده گرامی باشید"
      ],
      "metadata": {
        "id": "a_MsemZ-CVqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import csv\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "def save(filename, dictionary):\n",
        "    w = csv.writer(open(filename, \"w\"))\n",
        "    for key, val in dictionary.items():\n",
        "        w.writerow([key, val])\n",
        "\n",
        "\n",
        "def load(filename):\n",
        "    d = {}\n",
        "    with open(filename, mode='r') as infile:\n",
        "        reader = csv.reader(infile)\n",
        "        for rows in reader:\n",
        "            if len(rows) == 2:\n",
        "                d[rows[0]] = float(rows[1])\n",
        "    return d\n",
        "\n",
        "\n",
        "def cleaner(text):\n",
        "    with open('/content/drive/MyDrive/bayes/stop-words.txt', \"r\") as word_list:\n",
        "      words = word_list.read().split('\\n')\n",
        "    words.append(\"\\u200c\")\n",
        "    words.append(\"\\u200f\")\n",
        "    digits = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "    en = ['C','V','B','N','M','Q','W','E','R','T','Y','U','I','O','P','A','S','D','F','G','H','J','K','L','Z','X','q', 'w','e','r','t','y','u','i','o','p','a','s','d','f','g','h','j','k','l','z','x','c','v','b','n','m']\n",
        "    for q in digits:\n",
        "      words.append(str(q))\n",
        "    for e in en:\n",
        "      words.append(e)\n",
        "    words=set(words)\n",
        "    stopWords = words\n",
        "    text = text.replace('\\n', ' ')\n",
        "    mail = text.split(' ')\n",
        "    mail = [word for word in mail if word not in stopWords]\n",
        "    return mail\n",
        "\n",
        "def train(filename):\n",
        "    files = [f for f in listdir(filename) if isfile(join(filename, f))]\n",
        "    dictionary = {}\n",
        "    counter = 0\n",
        "    for file in files:\n",
        "        file = open(filename + \"/\" + file, errors=\"ignore\")\n",
        "        text = file.read()\n",
        "        file.close()\n",
        "        mail = cleaner(text)\n",
        "        for word in mail:\n",
        "            counter += 1\n",
        "            if word in dictionary:\n",
        "                dictionary[word] += 1\n",
        "            else:\n",
        "                dictionary[word] = 1\n",
        "    fileCount = len(files)\n",
        "    for word in dictionary.keys():\n",
        "        dictionary[word] = float(dictionary[word]) / fileCount\n",
        "    dictionary[\"fileCount\"] = fileCount\n",
        "    return dictionary\n",
        "\n",
        "\n",
        "def classify(input, diction, priori):\n",
        "    input = cleaner(input)\n",
        "    p = 1\n",
        "    for word in input:\n",
        "        if word in diction.keys():\n",
        "            p *= diction[word]\n",
        "        else:\n",
        "            p *= 1/diction[\"fileCount\"]+len(diction)+1\n",
        "    p *= priori\n",
        "    return p\n",
        "\n",
        "\n",
        "def trainNsave():\n",
        "    save(\"spam_train_results.csv\", train(\"spam\"))\n",
        "    save(\"ham_train_results.csv\", train(\"ham\"))\n",
        "\n",
        "\n",
        "def test():\n",
        "    hamDict = load(\"ham_train_results.csv\")\n",
        "    spamDict = load(\"spam_train_results.csv\")\n",
        "    files = [f for f in listdir(\"input\") if isfile(join(\"input\", f))]\n",
        "    spamC = hamC = 0\n",
        "    for file in files:\n",
        "        file = open(\"input\" + \"/\" + file, errors = \"ignore\")\n",
        "        text = file.read()\n",
        "        file.close()\n",
        "        ham = classify(text, hamDict, 0.5)\n",
        "        spam = classify(text, spamDict, 0.5)\n",
        "        if ham <= spam:\n",
        "            spamC += 1\n",
        "        else:\n",
        "            hamC += 1\n",
        "    print(\"Spam:\\t\" + str(hamC))\n",
        "    print(\"Ham:\\t\" + str(spamC))\n",
        "\n",
        "\n",
        "trainNsave()\n",
        "test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zgkudux5do2F",
        "outputId": "c218e1f8-45bf-450e-badc-eea249dc6b8b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spam:\t0\n",
            "Ham:\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚓"
      ],
      "metadata": {
        "id": "K9aPHztyAwTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "7IldpXGXArss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/bayes/spam-words.txt', \"r\") as word_list:\n",
        "  words = word_list.read().split('\\n')\n",
        "spamwords=set(words)"
      ],
      "metadata": {
        "id": "o4MLoeubu37O"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "من این قطعه کد را برای انتقال برخی از پیامک ها از پوشه اولیه پیامکا( که خودم اسمش رو هم یعنی غیر اسپم گذاشتم)، به پوشه اسپم نوشتم. برای اینکار هر پیامکی که تعداد کلمات اسپم ان بر اساس فایل کلمات اسپمی که شما دادید، احتمالش بیشتر از ۰.۶ میشد را میفرستادم تو پوشه اسپم. اینجوری یه دیتاست شامل دو پوشه اسپم و هم دارم که فایلهای تکست داخل انها شامل پیامک های به ترتیب اسپم  و هم هستند.\n",
        "دلیل این مدلی ایجاد دیتاستم این بود که باید داده هام برچسب گذاری میشدند به تفکیک.\n",
        "</div>"
      ],
      "metadata": {
        "id": "3shRUYsZ-qWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#حاصل اجرای این بخش ایجاد پوشه اسپم و هم بود به عنوان دیتاست\n",
        "\n",
        "import os, shutil\n",
        "path = \"/content/drive/MyDrive/bayes/ham/\"\n",
        "moveto = \"/content/drive/MyDrive/bayes/spam/\"\n",
        "files = os.listdir(path)\n",
        "files.sort()\n",
        "sp=0\n",
        "for f in files:\n",
        "    file = open(path+f, \"rt\")\n",
        "    data = file.read()\n",
        "    w = data.split()\n",
        "    n=len(w)\n",
        "    for i in w:\n",
        "      if i in spamwords:\n",
        "        sp=sp+1\n",
        "    p=sp//n\n",
        "    if p>0.6:\n",
        "      src = path+f\n",
        "      dst = moveto+f\n",
        "      shutil.move(src,dst)"
      ],
      "metadata": {
        "id": "LK3wEMxvtg2z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}