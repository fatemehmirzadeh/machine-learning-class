<div dir="rtl">
	
  ### سعی کنید دسته بندی هر خبر را به صورت خودکار بدست آورید.
  برای دسته بندی خودکار خبر ها نیاز داریم که بر اساس داده های موجود شامل خبر های مختلف با موضوعات گوناگون بر اساس یکی از الگوریتم های خوشه بندی تعدادی خوشه یا دسته به عنوان موضوعات محوری هر خبر تعیین شود . از الگوریتم kmean که کاربردی ترین الگوریتم خوشه بندی است با نعداد دسته 3 استفاده شده است.
  به این منظور از کتابخانه های pandas و matplotlib و sklearn برای دسترسی به داده مربوط به اخبار و وکتورایز کردن هر خبر و نهایتا خوشه بندی آن ها استفاده شده است.
  ابتدا داده را باز کرده به صورت رندم مرتب می کنیم و 90% داده ها را به عنوان داده train و 10% داده ها را به عنوان داده تست در نظر می گیریم.سپس با استفاده از دیتاست مربوط به stop word کلمات زاید هر خبر را حذف می کنیم.
</div>
<br/>
  
  ![](https://github.com/semnan-university-ai/machine-learning-class/blob/main/excersiecs/mahyaghlmrz/30/image/1.jpeg)
  
<br/>
<div dir="rtl">
	کلمات تکراری موجود در همه سطر های برابر 5619  می باشد و هر بردار به صورت یک بردار به طول 5619 در می آید که برای هر نمونه آرایه های این بردار یا وکتور تعداد تکرار هر کلمه را نشان می دهد.در واقع این وکتور ها که شامل کلمات بدنه اخبار هستند بردار ویژگی این cluster هستند.
</div>  

  ![](https://github.com/semnan-university-ai/machine-learning-class/blob/main/excersiecs/mahyaghlmrz/30/image/2.jpeg)
  
<br/>
<div dir="rtl">
	الگوریتم kmeans را از کتابخانه sklearn.cluster اضافه کرده و با 3 خوشه و محدودیت تکرار 20 بار بر روی داده train اجرا شده است.با کمک دستور fit prdict مراکز دسته ها تعیین شده است. مشخص است که دسته اول با برچسب0 مربوط به اخبار ورزشی ، دسته دوم با برچسب 1 مربوط به اخبار سیاسی و اقتصادی و دسته آخر با برچسب 2 مربوط به خبر های حوزه سلامت می باشد. 26 ویژگی  هر مرکز دسته تعیین شده است و داده های تست بر اساس همین مراکز دسته با کمک تابع predict برچسب زده می شود.
	صحت این روش را با تطبیق عکس مربوط به هر خبر و برچسب آن تایید می شود.
</div>
  
  ![](https://github.com/semnan-university-ai/machine-learning-class/blob/main/excersiecs/mahyaghlmrz/30/image/3.jpeg)
  
<br/>
<div dir="rtl">
  کد نهایی:
</div>

```
import pandas as pd 
import matplotlib.pyplot as plt
from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

data = pd.read_csv("news_data1.csv",error_bad_lines=False,usecols =["headline_text"])

# Randomize the dataset
data_randomized = data.sample(frac=1, random_state=1)

# Calculate index for split
training_test_index = round(len(data_randomized) * 0.9)

# Split into training and test sets
training_set = data_randomized[:training_test_index].reset_index(drop=True)
test_set = data_randomized[training_test_index:].reset_index(drop=True)

with open('stop-word.txt', 'r') as fd:
  stop_words_list = fd.read().split('\n') 
stop_words =frozenset(stop_words_list)

desc = training_set['headline_text'].values
vectorizer = TfidfVectorizer(stop_words = stop_words)
X = vectorizer.fit_transform(desc)
word_features = vectorizer.get_feature_names()

kmeans = KMeans(n_clusters = 3, n_init = 20) # n_init(number of iterations for clsutering) n_jobs(number of cpu cores to use)
#kmeans.fit(X)
kmeans.fit_predict(X)
common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]
for num, centroid in enumerate(common_words):
    print(str(num) + ' : ' + ', '.join(word_features[word] for word in centroid))

descTest = test_set['headline_text'].values
Y = vectorizer.transform(descTest)
kmeans.predict(Y)

```

  
  </div>
