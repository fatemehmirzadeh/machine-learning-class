# task-22

### overfit

<div    dir="rtl">

 فرض میکنیم یک از مثال ‌های آموزشی در اختیار داریم. و یک فرضیه داریم که کل این مثال‌های اموزشی را دسته بندی کرده و خطایی برای این فرضیه برای داده‌های آموزشی وجود ندارد. حال اگر به سمت داده های واقعی پیش برویم ، میبینیم که با در نظر گرفتن فرضیه دیگری مانند h1 دچار خطای کمتری نسبت به h شده است. به عبارت دیگر h داده های اموزشی من را overfitt کرده است. به عبارت دیگر یک فرضیه دیگر تحت عنوان h1 یافت شده است که روی مثال های آزمایشی دارای خطای کمتری نسبت داده های آموزشی است. یعنی در عمل کاربرد بهتری دارد. در واقعا overfitt نشان می دهد که فرضیه اولیه همان h بیش از حد به مثال های آموزشی وابسته شده است.
</div>

-------------------------------------

### local minumum

<div  dir="rtl">
 به تصویر زیر توجه می‌کنیم:
</div>
 
 ![image](https://user-images.githubusercontent.com/95109502/147107582-6989043a-f675-4b6b-afdc-c6ebb97fc723.png)

 <div  dir="rtl">
  در حل مسائل بهینه سازی قصد داریم یا به بیشترین مقدار تابع دست پیدا کنیم یا به کمترین مقدار تابع هدف. با توجه به عکسی که در بالا مشاهده می شود یک تابع هدف نشان دادهمی شود که بایستی  کمترین مقدار آن را بدست بیاورم. با استفاده از برخی روش های حل مسائل بهینه سازی مانند روش گاهشی گرادیان، از از بالای تابع شروع کنیم و در خلاف جهت گرادیان به سمت پایین حرکت کنیم، به یک نقطه میرسی که در ان نقطه مقدار تابع به ازای مقدار جدید کوچکتر از تابع به ازای مقدار قدیم نمیشود. و بر خلاف انتظار بیشتر می شود. در واقع این نشان می دهد که در یک چاله گرفتار شدیم که به این چاله یک مینیمم محلی یا local minimum گفته می شود. در بایستی با به کار گیری روشی از این مینیمم های محلی رهایی جوییم.
 </div>
 
 ----------------------------------
 
 ### Gradient descent
 
 <div   dir="rtl">
 این روش یکی از الگوریتم هایی است که برای حل مسائل بهینه سازی به کار گرفته می شود. ایده اصلی این روش به این صورت است که اگر روی دامنه کوه هستی برای رسیدن به ته دره، به  خلاف جهت شیب کوه حرکت کن.
 
 این مسأله کمینه سازی کاملا به صورت شهودی حل می شود. حال در ادامه با به نمایش گزاشتن برخی تصاویر به  صورت مبسوط به تشریح این روش می پردازیم.
 </div>
 
 <div  dir="rtl">
 به تصویر زیر توجه می‌کنیم:
</div>

![image](https://user-images.githubusercontent.com/95109502/147112831-bf098f2e-2919-4746-97e6-480c52423956.png)

<div   dir="rtl">
منحنی دارای یک نقطه کمینه است و ما آن را نمی دانیم. برای رسیدن به آن از یک نقطه تصادفی شروع می کنیم و به صورت زیر به سمت نقطه کمینه حرکت می کنیم:  
 </div>
 
 ![image](https://user-images.githubusercontent.com/95109502/147115175-d6d2321a-1761-4fab-893c-9423383fd633.png)

<div   dir="rtl">
 چون نقطه بهینه را نمیدانیم لذا نمیدانیم که بایستی به سمت چپ برویم یا سمت راست. لذا با استفاده از گرادیان تابع هدف میتوان مسیر حرکت را نتیجه گرفت. اگر گراذیان مثبت شود و داری شیب مثبت باشیم این نشان از این دارد که باید به سمت چپ حرکت کنیم و اگر گرادیان منفی باشد نشان از این دارد بایستی به سمت راست در حال حرکت باشیم. این توضیحات در عکس زیر نشان داده شده است
</div>

![image](https://user-images.githubusercontent.com/95109502/147116525-7cccaaea-6551-4b3d-9ffe-82c4f5a2d3c1.png)


<div  dir="rtl">
 اگر در نقطه هستیم که مشتق مقدار بزرگی دارد، پس از نقطه کمینه دوریم. باید قدم های زیادی برداریم. همچنین اگر در نقطه هستیم که مشتق مقدار کوچکی است، پس به نقطه کمینه نزدیکیم. باید قدم های کمی برداریم. در مرحله بعدی، در هر نقطه ای هستیم، در
خلاف علامت مشتق، با قدم هایی متناسب با مقدار مشتق حرکت می کنیم. از این رو به این روش، روش گرادیان شیب گفته می‌شود.
 </div>
 
 <div  dir="rtl">
 
 به کار گیری این روش تکراری در یادگیری ماشین به صورتی است که در هر تکرار از تمامی نمونه های آموزشی استفاده می کند.
 </div>
 
 ----------------------------------
 
 ### lazy and eager learning
 
 <div   dir="rtl">
 در روش eager وقتی مثال را ارائه می دهیم، فرض می کند از روی مثال ها فرضیه بسازد و حالا هر نمونه ی آموزشی از راه برسد به وسیله ی فرضیه از قبل تعیین شده عملیات یادگیری را انجام می دهد. اما در روش lazy روشی است که وقتی مثال ها به سیستم داده می شود، فرضیه خود را تعیین می کند. در واقع زمانی وارد عمل می شود که شما از آن سوال بپرسی که نمونه جدید به عنوان مثال مثبت است یا منفی . به عبارت دیگر از قبل فرضیه ای ندارد که بخواهد بر طبق آن یادگیری را انجام دهد در زمان رسیدن داده جدید. اما در eager فرضیه از قبل ساخته شده است و آماده پذیرش نمونه جدید است.  درخت تصمیم یک روش eager است و روش حذف کاندید هم یک روش lazy به شمار می رود.
 </div>
 
