# task-22

### overfit

<div    dir="rtl">

 فرض میکنیم یک از مثال ‌های آموزشی در اختیار داریم. و یک فرضیه داریم که کل این مثال‌های اموزشی را دسته بندی کرده و خطایی برای این فرضیه برای داده‌های آموزشی وجود ندارد. حال اگر به سمت داده های واقعی پیش برویم ، میبینیم که با در نظر گرفتن فرضیه دیگری مانند h1 دچار خطای کمتری نسبت به h شده است. به عبارت دیگر h داده های اموزشی من را overfitt کرده است. به عبارت دیگر یک فرضیه دیگر تحت عنوان h1 یافت شده است که روی مثال های آزمایشی دارای خطای کمتری نسبت داده های آموزشی است. یعنی در عمل کاربرد بهتری دارد. در واقعا overfitt نشان می دهد که فرضیه اولیه همان h بیش از حد به مثال های آموزشی وابسته شده است.
</div>

### local minumum

<div  dir="rtl">
 به تصویر زیر توجه می‌کنیم:
</div>
 
 ![image](https://user-images.githubusercontent.com/95109502/147107582-6989043a-f675-4b6b-afdc-c6ebb97fc723.png)

 <div  dir="rtl">
  در حل مسائل بهینه سازی قصد داریم یا به بیشترین مقدار تابع دست پیدا کنیم یا به کمترین مقدار تابع هدف. با توجه به عکسی که در بالا مشاهده می شود یک تابع هدف نشان دادهمی شود که بایستی  کمترین مقدار آن را بدست بیاورم. با استفاده از برخی روش های حل مسائل بهینه سازی مانند روش گاهشی گرادیان، از از بالای تابع شروع کنیم و در خلاف جهت گرادیان به سمت پایین حرکت کنیم، به یک نقطه میرسی که در ان نقطه مقدار تابع به ازای مقدار جدید کوچکتر از تابع به ازای مقدار قدیم نمیشود. و بر خلاف انتظار بیشتر می شود. در واقع این نشان می دهد که در یک چاله گرفتار شدیم که به این چاله یک مینیمم محلی یا local minimum گفته می شود. در بایستی با به کار گیری روشی از این مینیمم های محلی رهایی جوییم.
 </div>
 
 ### greadian desent 
 
 
