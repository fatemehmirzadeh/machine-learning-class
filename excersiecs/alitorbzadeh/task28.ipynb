{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div  dir=\"rtl\">\n",
        "ابتدا در این قسمت این کتابخانه را برای پیش پردازش نصب و فراخوانی میکنیم.\n",
        "<div>"
      ],
      "metadata": {
        "id": "8WItXQ8qd0w7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install parsivar"
      ],
      "metadata": {
        "id": "Q-raf6on83bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div  dir=\"rtl\">\n",
        "در بلوک زیر کتابخانه های مورد نیاز معرفی شده اند. پکیج اخر برای نوشتن نتیجه در فایل اکسل است و برای کار با اکسل استفاده شده است.\n",
        "<div>"
      ],
      "metadata": {
        "id": "L-CXwAxjeK5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from parsivar import Normalizer , Tokenizer , FindStems\n",
        "import csv\n",
        "from string import *\n",
        "import xlwt"
      ],
      "metadata": {
        "id": "BWOtgrSICqIL"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop = []\n",
        "spam=[]"
      ],
      "metadata": {
        "id": "qmolRv8PEsMs"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div   dir=\"rtl\">\n",
        "در دو بلوک بعدی متن کلمات توقف و متن کلمات اسپم را فراخوانی کردم و از این متن های برخی کارکتر های فاصله، نیم فاصله و رفتن به خط بعدی را که کد شده اسند را حذف کردم و در نهایت هر دو متن را به یک لیست تبدیل کرده ام.\n",
        "<div>"
      ],
      "metadata": {
        "id": "u4iQeYH9eiZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"stop-words.txt\", encoding=\"utf8\") as data:\n",
        "  for line in data:\n",
        "    stop.append(word)\n",
        "for u in range(0, len(stop)):\n",
        "    p = stop[u]\n",
        "    w = str(p).replace('\\\\u200c', ' ')\n",
        "    s = str(w).replace('\\\\u200f', ' ')\n",
        "    stop[u] = str(s).replace('\\n', '')"
      ],
      "metadata": {
        "id": "aoNYlenzPmEE"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"spam-words.txt\", encoding=\"utf8\") as q1:\n",
        "  for word1 in q1:\n",
        "    spam.append(word1)\n",
        "for u1 in range(0, len(spam)):\n",
        "    p1 = spam[u1]\n",
        "    r1 = str(p1).replace('\\\\u200c', ' ')\n",
        "    w1 = str(r1).replace('\\u200c', ' ')\n",
        "    s1 = str(w1).replace('\\\\u200f', ' ')\n",
        "    spam[u1] = str(s1).replace('\\n', '')\n",
        "print(len(spam))"
      ],
      "metadata": {
        "id": "1oCE1DIlP-CR",
        "outputId": "6bda23de-2608-47ea-9204-3c0496123a71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div  dir=\"rtl\">\n",
        "در این قسمت با استفاده از تابع normalize مواردی شامل شامل تبدیل ارقام فارسی به انگلیسی، ایجاد فاصله بین نقطه و کاراکترهای اطرافش و تبدیل فاصله به نیم فاصله تصحیح شد. سپس با استفاده از دستور تابع Tokenize_Words طی آن جمله به یکسری واژه (توکن) تبدیل می‌شود. در ادامه تعداد واژه های اسپم که در پیام مورد شمارش قرار داد و تعداد کل این کلمات را بع تعداد کل کلمات sms1 تقسیم کرده و از این طریف احتمال spam حاصل می شود. حال اگر این احتمال بیشتر از .5 شود این پیام در کلاس اسپم قرار میگیرد و اگر کمتر تر از نیم شود در کلاس not_spam قرار میگیرد. در انتها با استفاده از کتابخوانه xlwt نتایج را داخل یک فایل اکسل نوشته شده است و در فایلی تحت عنوان classification ذخیره می شود.\n",
        "<div>"
      ],
      "metadata": {
        "id": "7OV3KHeWe-6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"sms_data.txt\", \"r+\", encoding=\"utf16\") as K:\n",
        "  for i in K:\n",
        "    spamcounter=0\n",
        "    my_norm=Normalizer()\n",
        "    my_token=Tokenizer()\n",
        "    sms0=my_norm.normalize(i)\n",
        "    sms1=my_token.tokenize_words(sms0)\n",
        "    for i7 in stop:\n",
        "      if i7 not in sms1:\n",
        "        continue\n",
        "      else:\n",
        "        sms1.remove(i7)\n",
        "    for i in spam:\n",
        "      if i is  sms1:\n",
        "        spamcounter=spamcounter+1\n",
        "    prob_sp= spamcounter/len(sms1)\n",
        "    if prob_sp>.5 :\n",
        "      sms1.append(\"spam\")\n",
        "    else:\n",
        "      sms1.append(\"not_spam\")\n",
        "    result=xlwt.Workbook()\n",
        "    my_sheet=result.add_sheet('my data')\n",
        "    for i in range(len(sms1)):\n",
        "      my_sheet.write(0, i, sms1[i])\n",
        "    result.save('classification1.xlsx')\n"
      ],
      "metadata": {
        "id": "ydGImp5rIGug"
      },
      "execution_count": 45,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Welcome To Colaboratory",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}