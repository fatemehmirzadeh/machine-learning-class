<div dir="rtl">
سوال: مفاهیم زیر را به صورت خلاصه بررسی کنید.
<br/>  
Ovefitting
<br/>
Local minimum
<br/>
Gradient descent
<br/>
Eager and lazy learning
</div>
<br/>

<div dir="rtl">
Ovefitting
فرض کنید شما برای امتحان آخرِ ترم در حال درس خواندن هستید. استاد هم به شما ۱۰۰ عدد نمونه سوال داده است تا با استفاده از آن‌ها بتوانید خود را برای امتحان آماده کنید. اگر طوری مطالعه کنید که فقط این ۱۰۰ نمونه سوال را کامل بلد باشید و هر سوالِ دیگری که کمی از این ۱۰۰ سوال فاصله داشته باشد، اشتباه جواب دهید، یعنی ذهنِ شما بر روی سوالات آموزشی که استاد برای یادگیری داده است Overfit  یا بیش‌برازش شده است.  
</div>
<div dir="rtl">
 در دنیای الگوریتم‌ها Overfit شدن به معنای این است که الگوریتم فقط داده‌هایی که در مجموعه آموزشی (train set) یاد گرفته است را می‌تواند به درستی پیش‌بینی کند ولی اگر داده‌ای کمی از مجموعه‌ی آموزشی فاصله داشته باشد، الگوریتمی که Overfit شده باشد، نمی‌تواند به درستی پاسخی برای این داده‌های جدید پیدا کند و آن‌ها را با اشتباه زیادی طبقه‌بندی می کند.
</div>

![overfit](https://github.com/semnan-university-ai/machine-learning-class/blob/main/excersiecs/Homayontoosy/22/2.jpg)
</br>

<div dir="rtl">
 Local minimum
 فرض کنید در یک پاساژ مشغول خرید یک نوعِ خاص کفش هستید و در فروشگاه‌های مختلف، به دنبال فروشگاهی می‌گردید که ارزان‌ترین قیمت را برای آن کفش داشته بدهد. یکی یکی فروشگاه‌ها را سرکشی می‌کنید تا متوجه می‌شوید، مغازه‌های طبقه‌ی ۴ معمولاً اجناس را ارزان‌تر می‌فروشند. پس فروشگاه‌های طبقه‌ی ۴ را با دقتِ بیشتری می‌گردید تا بالاخره به فروشگاهی می‌رسید که ارزان‌ترین قیمت را در طبقه‌ی ۴ در آن پاساژ می‌دهد. در واقع شما به یک حالتِ بهینه دست پیدا کردید ولی این بهینگی مخصوصِ آن پاساژ بوده است. شاید اگر پاساژهای دیگر را می‌گشتید، قیمتی پایین‌تر و بهینه‌تر نیز پیدا می‌کردید. می‌توان گفت شما در یک بهینه‌ی محلی افتاده‌اید و ممکن است در پاساژهای دیگر، قیمت ‌های بهینه‌ تری نیز موجود باشد.
<br/>
<br/>
در دنیای الگوریتم‌ها نیز، وضع به همین صورت است. فرض کنید یک نرم‌افزاری مانند اینستاگرام می‌خواهد به شما یک دوست جدید برای دنبال کردن پیشنهاد دهد. اینستاگرام به دنبالِ این است که بهترین شخصِ مناسب شما را بهتان پیشنهاد دهد. در واقع به دنبال بهینه‌ی سراسری (بهترین شخص در کل اینستاگرام برای معرفی به شما) است. اما پیدا کردن بهترین شخص، نیاز به جستجوی تمامیِ اشخاص در اینستاگرام دارد که طبیعتا این کار در زمان معقولی میسر نیست. پس به جای آن اینستاگرام سعی می‌کند تا جایی که می‌تواند و در یک زمان معقول به دنبال شخصی که مناسب شما باشد بگردد و این کار باعث می‌شود بتواند یک جواب بهینه‌ی محلی را پیدا کرده و به عنوان یک پیشنهاد معقول به شما نمایش دهد.
</div>
 
<br/>
<div dir="rtl">
Gradient descent
بیایید با یک مثال توضیح دهیم. در مثال زیر به دنبال کمترین مقدارِ خطا می‌گردیم. با توجه به وزن‌ها کمترین میزان خطا در وزن ۱.۵ رخ داده است که مقدارِ آن برابر ۱ است. یعنی ما با کم و زیاد کردنِ مقدارِ وزن می‌خواهیم کمترین میزانِ خطا را مشخص کنیم. اما در شبکه‌های عصبی تعداد بسیار بیشتری وزن خواهیم داشت که بایستی به روز شوند. مثلاً در یک شبکه‌ی عصبی برای پردازش تصویر ممکن است تا ۱۰۰۰ یا بیشتر وزن داشته باشیم که در این صورت باید تابعِ خطا را با توجه به هر ۱۰۰۰ وزن مختلف ارزیابی کرده و سپس هر کدام از این وزن‌ها را تغییر داده و دوباره تست کنیم تا میزان خطا به دست آید. همان‌طور که تصور می بینید این عملیات بسیار وقت‌گیر و پرهزینه است. برای غلبه بر این مشکل از روشی به اسم کاهش گرادیان استفاده می‌شود.
فرض کنید به جای مثال بالا، نمودار مقادیر خطا برای وزن، به صورت زیر باشد: 
</div>

![overfit](https://github.com/semnan-university-ai/machine-learning-class/blob/main/excersiecs/Homayontoosy/22/3.jpg)

<div dir="rtl">
 
</div>

![overfit]()
