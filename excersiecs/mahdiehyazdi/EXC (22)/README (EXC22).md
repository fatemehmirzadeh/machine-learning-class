### مفاهیم زیر را به صورت خلاصه بررسی کنید.

- Ovefitting <br/>
<div dir="rtl" align="justify">
  
```
Ovefitting در عملکرد یادگیری بسیار خوب بوده، اما عملکرد آن بر روی مجموعه داده ای های دیگر (dataset) خوب نیست. 
```
  درواقع Over-fitting به این موضوع اشاره دارد که مدل ما بسیار خوب آموزش دیده است اما به خوبی تعمیم نیافته است. اجازه دهید با یک مثال واقعی پیش برویم.فرض کنید شما به یک کشور خارجی رفته اید و قصد دارید با تاکسی به جایی بروید و از بد روزگار راننده آن تاکسی شما را تلکه می کند و پول زیادی از شما می گیرد و در نهایت شما می گویید تمامی رانندگان تاکسی آن کشور دزد هستند. Over-generalization یا Over-fitting دقیقا همین چیزی است که در زندگی انسان ها در حال رخ دادن است و جالب است بدانید که ماشین نیز ممکن است در این دام بیفتد اگر به خوبی مراقب عملکردتان نباشید. بنابراین تعریف Over-fit را اینگونه می گوییم که بسیار خوب آموزش دیده است اما قدرت تعمیم پذیری یا Generalization ندارد.
  <br/>
  این اتفاق زمانی برایتان ممکن است بیفتد که مجموعه داده ای شما خیلی کوچک یا خیلی بزرگ و پیچیده باشد و همچنین شامل داده های نویزی نیز باشد ( البته کوچک بودن نیز به تنهایی می تواند مشکل ساز باشد حتی بدون داده های نویزی ). به همین خاطر می گوییم ماشین نمی تواند با داده های جدید درست نتیجه گیری کند.
</div>
Over-fitting = Good Learning + Not Generalized

زمانی که مدل را ساده تر می کنیم تا ریسک بیش برازش کمتر شود را Regularization یا تنظیم می گوییم.

- Local minimum
<div rtl="rtl" align="justify">
  
```
حداقل محلی یک تابع نقطه ای است که در آن مقدار تابع کوچکتر از نقاط نزدیک است، اما نسبت به تابع هدف بزرگتر است.
```
  برای مثال فرض کنید برای خرید ماسک به تعدادی داروخانه مراجعه میکنید از بین آنها ارزان ترین قیمت ماسک 5000 تومان است ولی  در داروخانه ای دیگر قیمت ماسک از 2000 تومان شروع میشود در این حالت از میان داروخانه ای از آنها گذر کرده اید ارزان ترین ماسک را خرید کرده اید ولی در مجموع ارزان ترین ماسک را نخریده اید به این حالت میتوان گفت مینیمم محلی زیرا در میان محله  ای که در آن ماسک را قیمت کرده اید بهینه ترین حالت را یافته اید ولی به داروخانه های قیمت مناسب تر سر نزده اید و بهینه ترین حالت را نیافته اید  

</div>


- Gradient descent

<div dir="rtl" align="justify">

```
الگوریتم گرادیان کاهشی، یک الگوریتم بهینه‌سازی تکراری مرتبه-اول هست که مینیموم محلی در یک تابع مشتق‌پذیر را پیدا می‌کند.
```
  
  قبل از اینکه توضیحات اصلی را شروع کنیم، بیایید چند نکته مقدماتی و لازم به شما بگویم.

توجه: حتما می‌دانید که در اینجا ما یک مینیمم داریم و آن هم یک مینیمم سراسری است. پس در اینجا با گرادیان کاهشی می‌توانیم به مینیموم سراسری برسیم.
  
گرادیان کاهشی الگوریتمی است که مینیموم‌های محلی را در یک تابع پیدا می‌کند.

می‌خواهم گرادیان کاهشی را طی یک مثال به شما توضیح بدهم. مثال زیر را درنظر بگیرید؛

  
  نکته یادتان باشد که یک تابع محدب (Convex) همواره یک اکسترمم دارد. اما یک تابع غیرمحدب (Non Convex) بیش از یک اکسترمم دارد. تفاوت یک تابع محدب و غیرمحدب چیست؟ معیار بررسی محدب/غیرمحدب بودن یک تابع، دو نقطه مانند A و B است! به شکل زیر نگاه کنید:

تابع محدب: هر دو نقطه‌ای روی منحنی انتخاب کنیم، به هیچ وجه خط واصل این دو نقطه، تابع را قطع نمی‌کند.
تابع غیرمحدب: دو نقطه‌ای وجود دارد که خط واصل آنها، تابع را قطع خواهد کرد. به دو علامت ضربدر در شکل سمت راست نگاه کنید که تابع را قطع کرده است.
</div>

- Eager and lazy learning
