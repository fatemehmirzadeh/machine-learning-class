<div dir="rtl">
4) مفهوم درخت تصمیم یا decision tree را با ذکر ویژگی های کلی آن نام ببرید.
<br/>
<br/>
درخت تصمیم از روی مثال های آموزشی به ما فرضیه ای  به شکل درخت  می دهد که انتظار داریم روی داده های واقعی (تست) هم درست جواب دهد.
<br/>
درخت تصمیم از یک سری گره و برگ تشکیل شده است، که به گره اول ریشه می گویند که در واقع مهم ترین ویژگی که دارای وزن و میزان تاثیر بیشتری در مسئله است  در ریشه قرار می گیرد
و در برگ ها هم کلاس  یا تصمیم قرار دارد.
<br/>
اینکه کدام ویژگی دارای اهمیت بیشتری است براساس خطا انتخاب می شود و آن ویژگی که خطای کمتری دارد در گره نخست قرار می گیردو ویژگی ها به ترتیب اهمیت از بالا به پایین چیده می شوند.
<br/>
فرضیه یا درختی برای ما مناسب تر است که دارای عمق کمتری باشد، در واقع درخت با عمق کمتر احتمال اورفیت شدن کمتری دارد.
<br/>
<br/>
برای انتخاب و بررسی ویژگی ها می توان به صورت زیر کار را انجام داد :

<br/>
   اگر X را ویژگی در نظر بگیریم:
 </div>

  <div dir="ltr">
  1. x : <br/>  
   False -> NO <br/> 
   True -> YES <br/>
  <br/>
  2. x : <br/>
   False -> YES <br/>  
   True -> NO <br/>
  </div>
  <br/>
  <div dir="rtl">
   دو فرضیه ساخته می شود که فرضیه ای که خطای کمتری دارد مد نظر است و در نهایت وقتی برای تمام ویژگی ها این خطا محاسبه شد ویژگی ای برنده است و انتخاب می شود که دارای مقدار خطای کمتری باشد.
   <br/>
   <br/>
  از معایب درخت تصمیم میتوان حساسیت زیاد نسبت به نویز و incremental (افزایشی) نبودن کسب دانش را نام برد.
 به عبارتی یادگیری تدریجی نخواهد بود یعنی اگر داده ای  به دادگان آموزشی اضافه شود  از ابتدا باید درخت و فرضیه ساخته شود. 
<br/>
   فرض مهمی که ما در زمان انتخاب ویژگی در نظر می گیریم برابر بودن  و یکسان  بودن هزینه برای تمام ویژگی هاست.
   <br/>
   درخت تصمیم برای ویژگی هایی مناسب است که دارای مقادیر گسسته هستند و برای مقادیر پیوسته مناسب نیست مگر اینکه با روش های گسسته سازی مقادیر پیوسته را به گسسته تبدیل کنیم.
   <br/>
   
