
<div dir="rtl">
  
 #### 23)  تفاوت های k=1 ، k=5 را در یک داده ی فرضی با الگوریتم knn بررسی کنید. 
  
  <br/>
  الگوریتم KNN روشی مبتنی بر نمونه است.
   این الگوریتم برچسب کلاسی نمونه ی مورد نظر را با توجه به برچسب همسایه های آن نمونه مشخص می کند.
  در واقع به آن نمونه هایی که شباهت بیشتری به آن ها دارد، تعلق می گیرد.
  معیار شباهت در این الگوریتم فاصله است و هرچه فاصله کمتر، شباهت بیشتر است. معیارهای مختلفی هم برای فاصله داریم مثل اقلیدسی وهمینگ و منهتن و....
   <br/>
  برای انتخاب مقدار K باید در نظر داشت افزایش K تاحدی حساسیت به نویز را کم می کند، از طرفی K بزرگتر از 9 هم  مرسوم نیست. 
  فرمولی پیشنهادی برای انتخاب K به اینصورت است که:
  <div/>
  <br/>
  (NUMBER OF SAMPLES)^ (1/2)
  
  <div dir="rtl">
    K=1 معمولا در نظر گرفته نمی شود، چون ممکن است نویز باشد  و از طرفی هم یک سری از داده ها هستند که ممکن است لیبل گذاری نشده باشند و باعث می شود در تصمیم گیری ها دچار اشتباه شود.  
     1nn روشی است که خیلی مستعد اتفاق ناخوشایند overfit شدن است. local شدن مشکل overfit دارد  و حتی اگر نویز باشد براساس آن تصمیم میگیریم  پس k کوچک احتمال overfit شدن  دارد
   مقدار K هم نباید زوج در نظر گرفت تا در رای گیری اکثریت، تساوی رخ ندهد.
  
  الگوریتم KNN الگوریتمی است که وابسته به پارامتر K است و با تغییر K میتواند حتی لیبل داده ی مورد نظر ما متفاوت شود.
  همانطور که در شکل زیر مشاهده می کنید اگر ما K=1انتخاب کنیم  نمونه ی مورد نظر ما (مربع قرمز) رنگ به یک نمونه ی ستاره نزدیک تر است،  پس لیبل داده ی مورد نظر ما  CLASS A که همان کلاس ستاره است خواهد شد.
  
  اما اگر در این الگوریتم ما K=5 در نظر بگیریم همانطور که مشهود است لیبل داده ی مورد نظر ما از CLASS 1 به CLASS 2 تغییر خواهد کرد چون به 3 داده ی سورمه ای و 2 ستاره نزدیک است و لیبل اکثریت CLASS 2 خواهد بود، پس تعیین دقیق پارامتر K خیلی مهم است و مبتنی بر فضای مساله باید انتخاب گردد. 
  
  مقدار K نمی تواند خیلی بزرگ انتخاب گردد چون مفهوم همسایگی زیر سوال می رود از طرفی هم نباید خیلی کوچک باشد 

 ![KNN.png](https://github.com/semnan-university-ai/machine-learning-class/blob/main/excersiecs/smahdimoghaddasi/EXC%20(23)/KNN.png)
    
 
