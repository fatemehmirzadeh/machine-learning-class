
<div dir="rtl">
  
 #### 23)  تفاوت های k=1 ، k=5 را در یک داده ی فرضی با الگوریتم knn بررسی کنید. 
  
  <br/>
  الگوریتم KNN برچسب کلاسی نمونه ی مورد نظر را با توجه به برچسب همسایه های آن نمونه مشخص می کند.
  در واقع به آن نمونه هایی که شباهت بیشتری به آن ها دارد، تعلق می گیرد. 
  برای انتخاب مقدار K باید در نظر داشت افزایش K تاحدی حساسیت به نویز را کم میکند.، از طرفی K بزرگتر از 9 هم در مسائل مرسوم نیست 
  فرمولی پیشنهادی برای انتخاب K   به اینصورت است که:
  <div/>
  <br/>
  (NUMBER OF SAMPLES)^ (1/2)
  
  <div dir="rtl">
     1nn روشی است که خیلی مستعد اتفاق ناخوشایند overfit شدن است. local شدن مشکل overfit دارد  و حتی اگر نویز باشد براساس آن تصمیم میگیریم  پس k کوچک احتمال overfit شدن بیشتر است
  با توجه به اینکه مقدار K هم نباید زوج در نظر گرفت تا در رای گیری اکثریت، تساوی رخ ندهد.
  
  الگوریتم KNN الگوریتمی است که وابسته به پارامتر K است و با تغییر K میتواند حتی لیبل داده ی مورد نظر ما متفاوت شود.
  همانطور که در شکل زیر مشاهده می کنید اگر ما K=3انتخاب کنیم  نمونه یمورد نظر ما داده ی سبز رنگ به دو نمونه دایرهی آبی و یک نمونه ی مربع زرد نزدیک تر است  پس لیبل داده یمورد نظر ما  CLASS A که همان کلاس داده های دایره ی آبی رنگ است خواهد شد.
  
  اما اگر در این الگوریتم ما K=5 در نظر بگیریم همانطور که مشهود است لیبل داده ی مورد نظر ما از A به B تغییر خواهد کرد چون به 3 داده یمربع زرد و 2 دایر هی آبی نزدیک است و لیبل اکثریت CLASS B خواهد بود، پس تعیین دقیق پارامتر K خیلی مهم است و مبتنی بر فضای مساله باید انتخاب گردد. 
  
  مقدار K نمی تواند خیلی بزرگ انتخاب گردد چون مفهوم همسایگی زیر سوال می رود از طرفی هم نباید خیلی کوچک باشد 

 ![KNN.png](https://github.com/semnan-university-ai/machine-learning-class/blob/main/excersiecs/smahdimoghaddasi/EXC%20(23)/KNN.png)
    
 ![23.png](https://github.com/semnan-university-ai/machine-learning-class/blob/main/excersiecs/smahdimoghaddasi/EXC%20(23)/23.png)
