

<div dir="rtl">
  
  ### 22) مفاهیم زیر را به صورت خلاصه بررسی کنید.
  <br/>
  
  ### Ovefitting
  ### Local minimum
  ### Gradient descent
  ### Eager and lazy learning
  <br/><br/>
  
  ### Ovefitting:
  <br/>
  
  در شکل 1 خط (فرضیه) روی مثال های آموزشی نسبتا خوب جواب می دهد و
  در شکل 2 فرضیه روی تمامی نمونه های آموزشی فیت شده است، و اگر نمونه ی تستی وارد شود فرضیه ی 1  نسبت به 2 بهتر عمل می کند، چون منطق بهتری دارد و به عبارتی داده های 
   آموزشي را حفظ نکره است. 
   <br/>
  فرضیه ای که روی مثال های آموزشی خیلی خوب عمل می کند، اما در مثال های آزمایشی (داده های واقعی ) دقت اش کم تر است، 
  در اين حالت، فرضیه روی مثال های آموزشی overfit شده است
  و به مثال های آموزشی خیلی خیلی بها داده ايم.
 
  <br/>
  
  ### شکل 1
  
  ![22-1.jpg](https://github.com/semnan-university-ai/machine-learning-class/blob/main/excersiecs/smahdimoghaddasi/EXC%20(22)/22-1.jpg)
       
  ### شکل 2
  
   ![22-2.jpg](https://github.com/semnan-university-ai/machine-learning-class/blob/main/excersiecs/smahdimoghaddasi/EXC%20(22)/22-2.jpg) 
    
   <br/>
  
   <div dir="rtl">
   دو نوع خطای فرضیه داریم خطای آموزش که خطا روی داده های آموزش (error train) است و خطای اصلی که خطا در کل توزیع (error D) است 
    <br/>  
  error D خطا در کل توزیع (خطای اصلی) برای ما مهم تر از error train است.
  <br/>
  
  
  error train (h) < error train (h`)  <br/>
  
  error D (h) > error D (h`)
  <br/>
  <br/>
  <div dir="rtl">
   در آموزش h بهتر عمل کرده است اما `h بهتر است. 
   <div/>
  
   D= کل توزیع(همه ی نمونه ها)
   <br/>
     <div dir="rtl"> 
   
 در شکل زیر میبینیم از یک جایی به بعد خطا زیاد شده است که همان جا باید trian شبکه متوقف گردد.
  <br/>
      
 ![22-3.png](https://github.com/semnan-university-ai/machine-learning-class/blob/main/excersiecs/smahdimoghaddasi/EXC%20(22)/22-3.png)
       
 ### Local minimum :  
       
 
در حالت ايده آل با تعيين يک نرخ يادگيري مناسب مي توان به خطاي حداقل رسيد ولي اگر نرخ يادگيري کم انتخاب شود، الگوريتم ممکن است در مينيمم هاي محلي گير کند 
(مينيمم محلي خواصي شبيه به مينيمم اصلي دارند و در اين مناطق نيز شيب خطا صفر است و الگوريتم به اشتباه فکر مي کند که به مقدار بهينه رسيده است)
و در نتيجه شبکه به درستي آموزش نمي بيند.
       
 ![22-4.png](https://github.com/semnan-university-ai/machine-learning-class/blob/main/excersiecs/smahdimoghaddasi/EXC%20(22)/22-4.png)
       
    

برای پرهیز از مینیمم محلی روشهای مختلفی وجود دارد:
       
افزودن ممنتم<br/>
استفاده از stochastic gradient descent
<br/>
استفاده ازشبکه های مختلف با مقادیر متفاوتی برای وزنهای اولیه

افزودن ممنتم:
می توان قانون تغییر وزن ها را طوری در نظر گرفت که تغییر وزن در تکرار n ام تا حدی به اندازه تغییر وزن در تکرار قبلی بستگی داشته باشد.
 <div/>
ΔWji (n) = η δj Xji + αΔWji (n-1)
<div dir="rtl">
    که در آن مقدارممنتم بصورت زیر می باشد:
           
0≤ α <1
        
<br/>     
افزودن ممنتم باعث می شود تا با حرکت در مسیر قبلی در سطح خطا:
<br/>
از گیر افتادن در مینیم محلی پرهیز شود
<br/>
از قرارگرفتن در سطوح صاف پرهیز شود
<br/>
با افزایش تدریجی مقدار پله تغییرات، سرعت جستجو افزایش یابد.
       
      
  
      
      
  

