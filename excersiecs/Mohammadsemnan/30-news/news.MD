منبع:https://www.kaggle.com/thebrownviking20/k-means-clustering-of-1-million-headlines

برای دسته بندی بیشتر میتوان تعداد خوشه ها را نیز افزایش داد
```
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from nltk.tokenize import RegexpTokenizer
from nltk.stem.snowball import SnowballStemmer
%matplotlib inline

data = pd.read_csv("/content/news_data.txt", encoding='utf-16')
data.info()

punc =[line.strip() for line in open('/content/stop-words.txt', encoding='utf-8')]
stop_words = text.ENGLISH_STOP_WORDS.union(punc)
vectorizer = TfidfVectorizer(stop_words = stop_words)
X = vectorizer.fit_transform(data)

word_features = vectorizer.get_feature_names()
print(len(word_features))
print(word_features[5000:5100])

stemmer = SnowballStemmer('arabic')
tokenizer = RegexpTokenizer(r'[ا-ی\']+')

def tokenize(text):
    return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())]

vectorizer2 = TfidfVectorizer(stop_words = stop_words)
X2 = vectorizer2.fit_transform(data)
word_features2 = vectorizer2.get_feature_names()
print(len(word_features2))
print(word_features2[:50]) 

vectorizer3 = TfidfVectorizer(stop_words = stop_words, max_features = 1000)
X3 = vectorizer3.fit_transform(data)
words = vectorizer3.get_feature_names()

from sklearn.cluster import KMeans
wcss = []

kmeans = KMeans(n_clusters = 1, n_init = 20)
kmeans.fit(X3)
# Finally, we look at 8 the clusters generated by k-means.
common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]
for num, centroid in enumerate(common_words):
    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))
```
